[
{
	"uri": "http://localhost:1313/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Quách Nguyễn Chí Hùng\nPhone Number: 076551890\nEmail: bacon3632@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/09/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.01-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "OJT E-commerce Architecture OJT E-commerce is a modern serverless e-commerce platform built entirely on AWS. The architecture follows best practices for scalability, security, and cost optimization, replacing traditional Spring Boot backend with AWS Lambda functions.\nKey Components Frontend: React + Vite application hosted on S3 with CloudFront CDN Backend: Serverless API using API Gateway with 11 Lambda modules (63 endpoints) Database: RDS SQL Server Express 2019 in private subnet Storage: S3 buckets for images and frontend static files Authentication: JWT-based authentication with Cognito User Pool (optional) Security: VPC with public/private subnets, Security Groups, Secrets Manager Monitoring: CloudWatch Dashboard, Log Groups, and Alarms Architecture Diagram Workshop Flow This workshop follows a practical application development workflow:\nSetup Environment - Install tools (Node.js, AWS CLI, CDK CLI) CDK Bootstrap - Prepare AWS account for CDK deployments Deploy Core Infrastructure - VPC, RDS, S3, Cognito (NetworkStack, StorageStack, AuthStack, DatabaseStack) Deploy API Stack - API Gateway + Placeholder Lambda functions Deploy Lambda Code - Deploy actual Lambda function code (63 APIs) Deploy Frontend - Build React app and deploy to S3 + CloudFront Deploy Monitoring - CloudWatch Dashboard and Alarms Test Endpoints - Verify all 63 API endpoints work end-to-end Monitor \u0026amp; Maintain - Use CloudWatch for monitoring and debugging What You\u0026rsquo;ll Learn Infrastructure as Code with AWS CDK (TypeScript) Serverless architecture replacing Spring Boot with Lambda 2-step deployment strategy: Infrastructure + Lambda code separation RDS SQL Server in private subnet with Secrets Manager CloudFront CDN with Origin Access Control (OAC) JWT authentication with optional Cognito integration Lambda function modular organization (11 modules: Auth, Products, ProductDetails, Cart, Orders, Categories, Brands, Banners, Ratings, Users, Images) VPC design with NAT Gateway for private subnet internet access CloudWatch monitoring with Dashboard and Alarms Cost optimization strategies for development environment Realistic Cost Estimate Development Environment (Optimized):\nEstimated Monthly Cost: $44/month (60% reduction from original $111/month) Cost Breakdown:\nService Configuration Monthly Cost NAT Gateway 1 instance $23 RDS SQL Server t3.micro $15 Lambda 11 modules, 128MB $2 S3 Storage Images + Frontend $1.25 CloudFront CDN distribution $1.50 CloudWatch Dashboard + Logs $1.50 Total ~$44/month Cost Optimization Applied:\nRDS instance size: t3.small → t3.micro (saves $39/month) NAT Gateway: 2 → 1 instance (saves $23/month) Lambda memory: 256MB → 128MB (saves 50% per invocation) Lambda timeout: 30s → 10s (faster execution) Log retention: 7 days → 1 day (saves 85% CloudWatch cost) Backup retention: 7 days → 1 day for development Free Tier Benefits (First 12 months):\nLambda: 1M requests/month free S3: 5GB storage + 20K GET requests free RDS: t3.micro 750 hours/month free (single-AZ) Key Features E-commerce Platform: Products, Cart, Orders, Categories, Brands 63 API Endpoints: Complete CRUD operations for all modules Image Upload: S3 integration for product images Search \u0026amp; Filter: Products by category, brand, price range Order Management: Create, track, and manage orders Rating System: Product ratings and statistics Banner Management: Dynamic banners for promotions Admin Functions: User management, order status updates Lambda Modules Summary Module Functions Description Auth 4 Login, Signup, Logout, Me Products 12 CRUD, Search, Filter, Best-selling, Newest ProductDetails 7 CRUD, Images upload Cart 6 Add, Get, Update, Remove, Clear, Count Orders 9 CRUD, COD, Status, Date-range filter Categories 6 CRUD, Search Brands 5 CRUD Banners 7 CRUD, Toggle Ratings 3 Get, Stats, Create Users 3 GetAll, GetById, UpdateProfile Images 1 Upload to S3 Total 63 Deployment Strategy 2-Step Deployment Process:\nDeploy Infrastructure (CDK) - 5-10 minutes\nVPC, Subnets, NAT Gateway RDS SQL Server + Secrets Manager S3 Buckets (Images, Frontend) API Gateway + Placeholder Lambda Cognito User Pool (optional) Deploy Lambda Code - 1-2 minutes\nPackage Lambda functions with dependencies Upload to AWS Lambda Update function code independently Benefits of Separation:\nCDK deploy faster (no Lambda code build) No dependency resolution errors Update Lambda code independently (30 seconds) Clear separation: Infrastructure vs Application code CI/CD friendly deployment "
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "OpenAI open weight models now available on AWS On August 5, 2025, AWS announced that two of OpenAI’s latest open-weight models — gpt-oss-120b and gpt-oss-20b — are now accessible through Amazon Bedrock and Amazon SageMaker JumpStart. This release marks a significant milestone, as it’s the first time OpenAI has provided public access to model weights since GPT-2, opening the door to greater flexibility and customization.\nTask Specialization The new OpenAI open-weight models are designed with specific use cases in mind. They excel at coding tasks, scientific data analysis, and mathematical reasoning, making them well-suited for industries and teams that require advanced technical problem-solving capabilities.\nExtended Context Both models support an expanded context window of 128,000 tokens. This allows them to process very long documents, conversations, or codebases in a single run. For applications involving legal analysis, research papers, or extended dialogues, this large context size provides a clear advantage.\nDeployment Options AWS offers two main paths for deploying these models. With Amazon Bedrock, customers can access the models as a fully managed service, avoiding the need to handle infrastructure. On the other hand, SageMaker JumpStart allows for more hands-on control, enabling exploration, deployment, and fine-tuning either through SageMaker Studio or the Python SDK. This dual approach caters to both convenience and flexibility, depending on the user’s preference.\nSecurity Controls Security remains central to AWS’s offering. Customers can deploy models within a private VPC, ensuring data isolation and stronger protection. AWS also provides Guardrails to help organizations maintain responsible use of AI by filtering outputs, applying compliance rules, and preventing harmful responses. These built-in controls give businesses more confidence in deploying open-weight models safely.\nPerformance Claims According to AWS, the gpt-oss-120b model delivers significant efficiency gains when run on Bedrock. The company claims it is three times more cost-efficient than Gemini, five times more efficient than DeepSeek-R1, and twice as efficient as OpenAI’s own o4 model. While these benchmarks are impressive, they are based on AWS’s internal testing. As with any marketing claim, organizations are advised to validate performance against their own workloads before relying on these figures.\nTransparency with Chain-of-Thought Another notable capability is the option to output chain-of-thought reasoning traces. This feature allows users to see the step-by-step reasoning process behind a model’s response. For applications requiring explainability or verification, this can be a valuable tool. However, in practice, such reasoning outputs may add complexity and not always be suitable for all production environments.\nLimitations and Considerations Despite their potential, the models come with certain limitations. At launch, they are only available in a few AWS regions: US West (Oregon) for Bedrock, and US East (Ohio, Virginia) as well as Asia (Mumbai, Tokyo) for SageMaker JumpStart. This restricted availability may slow adoption for organizations operating in other parts of the world.\nAdditionally, while open weights allow for deep customization and fine-tuning, they also shift responsibility onto the user. Businesses must take care to implement proper safety measures, manage compliance requirements, and guard against misuse. In short, the openness of the models provides freedom but demands responsibility.\nConclusion The release of OpenAI’s gpt-oss-120b and gpt-oss-20b on AWS represents a major step in the evolution of AI accessibility. By combining advanced reasoning capabilities, extended context handling, and open-weight customization with the convenience of Bedrock and the flexibility of SageMaker, AWS is positioning itself as a powerful platform for AI innovation.\nHowever, I think customers should remain cautious. Regional limitations, marketing-heavy performance claims, and the added responsibilities of managing open weights all require careful consideration. With proper validation and governance, these models could become valuable assets for organizations seeking both transparency and control in their AI systems.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "OpenSecrets uses AWS to transform political transparency through enhanced data matching OpenSecrets is a nonpartisan, independent nonprofit organization whose mission is to serve as the trusted authority on money in American politics. It pursues this mission by providing comprehensive and reliable data, analysis, and tools for policymakers, storytellers, and citizens. Its vision is for Americans to use data on money in politics to create a more vibrant, representative, and responsive democracy.\nThrough the AWS Imagine Grant—a public grant program that provides both cash and Amazon Web Services (AWS) credit funding to registered nonprofit organizations that are using cloud technology to accelerate their missions—OpenSecrets embarked on an ambitious project to revolutionize its political contribution database. The project focused on enhancing donor matching accuracy and efficiency through advanced data processing techniques. The improved system empowers more citizens and organizations to hold political systems accountable by making political finance data more accurate and accessible than ever before.\nWrestling with inconsistent political data Political contribution data arrives from multiple sources with varying formats, naming conventions, and data quality standards. This created a massive challenge for researchers, journalists, and citizens trying to track money in politics accurately.\nThe complexity of political finance data The challenge begins with the sheer diversity of data sources. Federal Election Commission (FEC) filings arrive in different formats, state election commissions each have their own reporting standards, and lobbying disclosure forms follow yet another set of conventions. Individual donors might be listed as \u0026ldquo;John Smith,\u0026rdquo; \u0026ldquo;J. Smith,\u0026rdquo; \u0026ldquo;John A. Smith,\u0026rdquo; or \u0026ldquo;Smith, John\u0026rdquo; across different filings, making it nearly impossible to track their complete contribution history without sophisticated matching algorithms.\nCorporate entities present an even greater challenge. A single company might appear in records under its full legal name, common trade name, various subsidiary names, or even through different political action committees (PACs). For example, a technology company might contribute through \u0026ldquo;ABC Corp,\u0026rdquo; \u0026ldquo;ABC Corporation,\u0026rdquo; \u0026ldquo;ABC Technology Solutions,\u0026rdquo; or \u0026ldquo;ABC PAC,\u0026rdquo; making it difficult to aggregate the true scope of corporate political influence.\nManual processes reaching breaking point The OpenSecrets team was spending disproportionate amounts of time cleaning and reconciling data rather than analyzing it for meaningful insights. Staff members would manually review potential matches, cross-reference names across databases, and verify identities through public records—a process that could take hours for complex cases involving common names or large corporate entities.\nThis manual process was not only time-intensive, but also prone to human error—potentially compromising the accuracy of the organization\u0026rsquo;s data. A single misidentified donor could skew analysis of contribution patterns, while missed connections between entities could obscure important relationships in political financing networks.\nThe challenge was particularly urgent because political finance data grows exponentially during election cycles. During the 2024 election cycle, for instance, OpenSecrets processed over 500 million contribution records, compared to roughly 200 million during off-year periods. Manual processing was becoming increasingly unsustainable as both the volume and velocity of incoming data continued to accelerate.\nThe stakes of data accuracy Without an automated solution, OpenSecrets risked falling behind in its mission to provide timely, accurate information about campaign funding and lobbying activities. Inaccurate or incomplete data could mislead journalists writing investigative stories, researchers conducting academic studies, or citizens trying to understand their representatives\u0026rsquo; funding sources.\nThe organization needed a system that could handle hundreds of millions of records while maintaining the high precision standards required for political transparency work. False positives in donor matching could incorrectly attribute contributions, while false negatives could hide important patterns of political influence—both scenarios undermining the organization\u0026rsquo;s credibility and mission.\nBuilding a scalable data matching solution OpenSecrets initially proposed using machine learning for entity resolution, but as the project progressed, the team shifted to a more deterministic approach that better served their specific needs. They decided to use AWS-hosted Snowflake for data processing and AWS-hosted Elasticsearch for entity matching and scoring.\nFrom machine learning to deterministic matching The initial machine learning approach, while technically sophisticated, presented several challenges for OpenSecrets\u0026rsquo; specific use case. Black box algorithms made it difficult for staff to understand why certain matches were made, creating trust issues when explaining methodology to external researchers and journalists. Additionally, the training data requirements for ML models were substantial, and the organization needed results that could be audited and explained to maintain credibility in political transparency work.\nThe shift to a deterministic, rule-based approach provided several key advantages:\nTransparency: Every matching decision could be traced back to specific rules and scoring criteria Explainability: Staff could articulate to external users exactly how matches were determined Flexibility: Rules could be adjusted based on domain expertise without retraining models Speed: Deterministic algorithms could process records faster than complex ML inference Technical architecture and AWS services Running both Snowflake and Elasticsearch on AWS provided OpenSecrets with the scalability, speed, and centralized infrastructure necessary to handle its massive datasets. The architecture leveraged several key AWS services:\nAmazon EC2 instances powered the Elasticsearch clusters, providing the computational resources needed for real-time search and matching operations. The team configured auto-scaling groups to handle varying workloads during peak processing periods, such as FEC filing deadlines when thousands of new records might arrive simultaneously.\nAmazon S3 served as the primary data lake, storing raw contribution files, processed datasets, and backup copies of historical data. The team implemented lifecycle policies to automatically transition older data to cheaper storage classes while maintaining accessibility for historical analysis.\nAWS Lambda functions handled data preprocessing tasks, cleaning incoming files and standardizing formats before they entered the main processing pipeline. This serverless approach allowed the team to process incoming data streams without maintaining dedicated infrastructure.\nAmazon RDS provided relational database services for storing processed results and maintaining reference tables for name standardization and entity relationships.\nElasticsearch implementation details The Elasticsearch implementation formed the heart of the matching system. The team created sophisticated indexing strategies that enabled fuzzy matching across multiple fields simultaneously. Key features included:\nPhonetic matching using Soundex and Metaphone algorithms to catch name variations like \u0026ldquo;Smith\u0026rdquo; vs \u0026ldquo;Smyth\u0026rdquo; or \u0026ldquo;Catherine\u0026rdquo; vs \u0026ldquo;Katherine.\u0026rdquo;\nNormalized scoring that weighted different types of matches based on their reliability. Exact Social Security Number matches received the highest scores, while fuzzy name matches were weighted based on the rarity of the name and the quality of supporting information.\nGeographic clustering that increased match confidence when donors shared addresses, ZIP codes, or employer information, helping distinguish between different individuals with common names.\nData processing workflow The AWS infrastructure allowed researchers and the tech team to process hundreds of millions of records efficiently while maintaining the flexibility to adapt their approach as they learned more about their data challenges. The complete workflow involved several stages:\nData ingestion: Raw files uploaded to S3 triggers Lambda functions for initial processing Normalization: Names, addresses, and employer information standardized using reference databases Elasticsearch indexing: Processed records indexed with multiple search strategies Matching execution: Batch jobs run matching algorithms across the entire dataset Scoring and ranking: Potential matches scored and ranked by confidence level Human review queue: Low-confidence matches flagged for manual verification Result storage: Confirmed matches stored in RDS for subsequent analysis The approach they ultimately chose offered several advantages over their original machine learning proposal. It provided faster development cycles, transparent logic that their team could understand and explain, and the ability to score and rank potential matches. This scoring system allows uncertain results to be flagged for human review, allowing the automated process to enhance—rather than replace—human expertise.\nPerformance optimizations The team implemented several performance optimizations to handle the scale of political finance data:\nParallel processing distributed matching tasks across multiple Elasticsearch nodes, reducing processing time from weeks to days for complete dataset refresh.\nIncremental updates allowed new records to be matched against existing data without reprocessing the entire database, enabling near real-time updates during active filing periods.\nCaching strategies stored frequently accessed results in Amazon ElastiCache, reducing response times for common queries and dashboard updates.\nTransforming political finance research The new system matches hundreds of millions of records with greater accuracy, automating entity resolution while flagging records with insufficient confidence for human review. This leap in data processing improves the quality of key public datasets, allowing researchers to focus on analysis rather than data cleaning, and enabling deeper insights into campaign funding and lobbying at the federal and state levels.\nQuantifiable improvements in data quality The transformation delivered measurable improvements across multiple dimensions of data quality:\nMatch accuracy increased by 85% compared to the previous manual process, with false positive rates dropping below 2%. This improvement was particularly notable for corporate entities, where the system successfully identified subsidiary relationships and PAC connections that had previously required hours of manual research.\nProcessing time decreased by 95%, with complete dataset refreshes now taking days rather than months. During peak election periods, the system can process over 100,000 new contribution records daily, compared to the previous capacity of roughly 5,000 records per day through manual processes.\nCoverage completeness improved by 40%, as the automated system could identify subtle connections that human reviewers might miss due to fatigue or time constraints. This includes detecting relationships between donors who use different name variations across multiple election cycles or contribution types.\nEnhanced analytical capabilities The improved data quality unlocked new analytical possibilities that were previously impossible due to data inconsistencies:\nLongitudinal donor tracking now allows researchers to follow individual donors\u0026rsquo; political giving patterns across multiple election cycles, revealing trends in political engagement and allegiance shifts. This capability has already enabled several groundbreaking studies on donor behavior and political polarization.\nCorporate network analysis can map complex relationships between parent companies, subsidiaries, and associated PACs, providing a more complete picture of corporate political influence. The system can automatically identify when a single corporate entity is contributing through multiple channels, helping journalists and researchers understand the true scope of business involvement in politics.\nGeographic clustering analysis reveals regional patterns in political giving, helping researchers understand how local economic conditions, industry presence, and demographic factors influence political contributions. This has proven particularly valuable for studies on the intersection of economic and political power.\nImpact on stakeholder workflows The enhanced data quality enables journalists to write more accurate stories with greater depth and nuance. Reporters can now quickly identify all contributions from a particular individual or corporate network without spending days on manual research. Several Pulitzer Prize-winning investigations have already leveraged the improved OpenSecrets data to expose previously hidden patterns of political influence.\nResearchers to conduct reliable studies with larger sample sizes and greater confidence in their findings. Academic institutions have reported that studies using OpenSecrets data now require significantly less time for data preparation, allowing more resources to be devoted to analysis and interpretation.\nCitizens to make informed decisions about political candidates through more accessible and comprehensive information. The improved data powers user-friendly tools on the OpenSecrets website, including interactive donor maps, contribution timelines, and influence network visualizations that make complex political finance data understandable to general audiences.\nScaling for future growth OpenSecrets\u0026rsquo; transformation supports democracy through increased transparency, while the system\u0026rsquo;s scalability meets growing demands for political transparency tools as it continues to expand for new data sources. The AWS infrastructure can easily accommodate:\nState and local data integration: The system is already processing campaign finance data from 15 additional states, with plans to expand to all 50 states over the next two years. Each new data source requires minimal infrastructure changes, as the flexible architecture can accommodate varying data formats and filing requirements.\nReal-time processing capabilities: During major election cycles, the system can provide near real-time updates as new filings are submitted to election commissions, enabling journalists to report on contribution patterns as they develop rather than waiting for quarterly summaries.\nInternational expansion potential: The underlying technology stack could be adapted to process political finance data from other democratic countries, supporting global transparency initiatives and comparative political research.\nDemocratizing access to political data Perhaps most significantly, the transformation has democratized access to political finance information. Previously, only well-resourced news organizations or academic institutions could afford the staff time required to conduct comprehensive political finance research. Now, smaller news outlets, civic organizations, and individual researchers can access high-quality, processed data through OpenSecrets\u0026rsquo; APIs and bulk download services.\nThis democratization has led to a proliferation of transparency initiatives at local and state levels, as community organizations can now easily analyze their local political funding patterns and hold elected officials accountable for their funding sources.\nLessons for nonprofit technology implementation OpenSecrets\u0026rsquo; experience offers valuable guidance for other nonprofits embarking on technology transformation projects. The leadership team\u0026rsquo;s first piece of advice is to be flexible, as the original plan might not be the best path once you\u0026rsquo;re deep in the work.\nEmbrace iterative development over perfect planning The OpenSecrets transformation demonstrates the value of agile methodology in nonprofit technology projects. Rather than spending months creating detailed specifications, the team started with a minimum viable product (MVP) approach that could process a subset of their data and provide immediate value.\nJacob Hileman, OpenSecrets\u0026rsquo; IT Director, explains: \u0026ldquo;We learned that trying to solve everything at once was actually slowing us down. By focusing on getting one piece working really well first, we could validate our approach and build confidence with stakeholders before tackling more complex challenges.\u0026rdquo;\nThis iterative approach allowed the team to:\nTest assumptions early with real data rather than theoretical scenarios Build stakeholder buy-in by demonstrating concrete results quickly Identify unforeseen challenges before they became major roadblocks Adapt to changing requirements as the organization\u0026rsquo;s needs evolved during the project Prioritize explainability over sophistication The OpenSecrets team also emphasizes the importance of building with users in mind. The team needed clear, explainable match logic, not a black box solution. This user-centered approach led to the development of a final system that could be trusted and effectively utilized by OpenSecrets staff and external partners.\nTrust building was crucial because OpenSecrets\u0026rsquo; reputation depends on data accuracy and transparency. Staff members needed to understand how matches were made so they could explain methodology to journalists, researchers, and the public. External users needed confidence that the underlying algorithms were sound and free from bias.\nThe decision to abandon machine learning in favor of rule-based matching exemplifies this principle. While ML approaches might have achieved slightly higher accuracy in some cases, the deterministic system provided:\nComplete auditability of every matching decision Adjustable parameters that domain experts could fine-tune Explainable results that could be defended in public forums Reproducible processes that external researchers could validate Leverage cloud infrastructure for flexibility Perhaps most importantly, OpenSecrets learned not to wait for perfection. The team recommends launching, learning, and refining iteratively. Running everything on AWS made it easier to pivot quickly without re-architecting their entire system, enabling them to adapt their approach based on real-world testing and feedback.\nCloud-first architecture proved essential for managing uncertainty in the project scope. As the team learned more about their data challenges, they could:\nScale resources up or down based on processing needs without capital investment Experiment with different services (Elasticsearch, various database engines, etc.) without infrastructure commitments Deploy updates rapidly through automated CI/CD pipelines Rollback changes quickly if new approaches didn\u0026rsquo;t work as expected Plan for change management and user adoption One unexpected lesson involved change management within the organization. Staff members who had spent years developing expertise in manual data processing initially viewed the automated system with skepticism. The team learned that user buy-in required more than just technical success—it needed careful communication and training.\nSuccessful adoption strategies included:\nInvolving staff in algorithm development by having them validate match results and suggest improvements Training sessions that helped staff understand how to use the new tools effectively Gradual transition rather than abrupt replacement of existing workflows Clear documentation that helped staff troubleshoot issues independently Build partnerships and leverage grant opportunities The AWS Imagine Grant proved crucial not just for funding, but for creating accountability and external validation of the project\u0026rsquo;s importance. The grant application process forced the team to clearly articulate their goals and success metrics, while the public nature of the grant created positive pressure to deliver results.\nOther nonprofits should consider:\nApplying for multiple grants to diversify funding and reduce risk Building relationships with technology partners who understand nonprofit constraints Documenting successes to support future grant applications and inspire other organizations Sharing lessons learned with the broader nonprofit technology community Measure impact beyond technical metrics While technical metrics like processing speed and accuracy were important, OpenSecrets learned to also measure mission impact. The true success of the project wasn\u0026rsquo;t just in the technology itself, but in how it enabled the organization to better serve democracy.\nKey impact measures included:\nIncreased media coverage using OpenSecrets data in investigative stories Academic research publications leveraging the improved datasets Citizen engagement metrics through website usage and API adoption Policy discussions informed by more comprehensive political finance analysis Organizational capacity freed up by automation to focus on higher-value work Key takeaways for organizations Flexibility is crucial: Be prepared to adapt your original plan as you learn more about your challenges User-centered design: Build solutions that your team can understand, trust, and effectively use Iterative approach: Launch early, learn from real-world usage, and refine continuously Cloud infrastructure advantage: AWS enabled quick pivots and scaling without major re-architecture Human-AI collaboration: The best solutions enhance rather than replace human expertise Supporting democracy through technology Democracy requires political accountability, which can only be achieved through transparency. OpenSecrets supports these core tenets of our political system by providing comprehensive and reliable data, analysis and tools for policymakers, storytellers, and citizens. The organization\u0026rsquo;s transformation demonstrates how cloud technology can amplify the impact of nonprofit missions, creating more accessible and accurate information for democratic participation.\nExpanding the transparency ecosystem The enhanced system empowers researchers, journalists, and citizens alike to better understand the flow of money in politics, ultimately contributing to a more informed and engaged democratic society. But the impact extends far beyond individual users—it\u0026rsquo;s creating a network effect that strengthens democratic institutions.\nAcademic institutions are now incorporating OpenSecrets data into curriculum development, teaching students to analyze political finance patterns as part of civics and political science education. Universities report that students can now complete meaningful research projects using political finance data, whereas previously such projects required graduate-level resources.\nNews organizations are developing automated alerts that notify reporters when unusual contribution patterns emerge, enabling more timely coverage of political finance stories. Local news outlets, in particular, have benefited from the ability to quickly analyze their representatives\u0026rsquo; funding sources without requiring dedicated data teams.\nCivic technology organizations are building downstream applications that make political finance data even more accessible to general audiences. These include mobile apps that let citizens scan QR codes on political advertisements to see funding sources, browser extensions that add contribution information to news articles, and social media bots that provide context on political spending in real-time.\nGlobal implications for democratic transparency The success of OpenSecrets\u0026rsquo; transformation has attracted attention from international transparency organizations seeking to improve political finance oversight in other democracies. Several countries are now exploring similar approaches to automate campaign finance analysis, potentially creating a global network of political transparency initiatives.\nThe European Union has expressed interest in adapting the OpenSecrets methodology for cross-border political finance tracking, particularly given concerns about foreign influence in elections. The technical architecture developed with AWS could potentially scale to handle multi-jurisdictional data with different languages and regulatory frameworks.\nEmerging democracies are particularly interested in the cost-effective approach, as many cannot afford large teams of data analysts but need effective tools to monitor political finance compliance and detect corruption patterns.\nTechnology as a force for democratic renewal The OpenSecrets transformation illustrates how strategic technology investments can strengthen democratic institutions at a time when trust in government and media is declining. By making political finance data more accessible and reliable, the project contributes to several important trends:\nData-driven journalism is becoming more prevalent as reporters gain access to better tools and datasets. This shift toward evidence-based reporting helps counter misinformation and provides citizens with more factual foundation for political discussions.\nCitizen oversight capabilities are expanding as individuals and grassroots organizations gain access to tools previously available only to well-funded institutions. This democratization of oversight tools creates more distributed accountability mechanisms.\nAcademic research on political finance is accelerating as researchers can focus on analysis rather than data collection and cleaning. This is leading to better understanding of how money influences political outcomes and more evidence-based policy recommendations.\nFuture directions and sustainability Looking ahead, OpenSecrets plans to leverage its AWS infrastructure for several innovative expansions:\nPredictive analytics could help identify potentially illegal contribution patterns or coordination between supposedly independent political actors. By analyzing historical patterns, the system might flag unusual activity for human investigation.\nNetwork analysis tools will map complex relationships between donors, candidates, and political organizations, revealing influence networks that might not be apparent from individual contribution records.\nIntegration with lobbying data will provide more comprehensive pictures of organizational influence by connecting campaign contributions with lobbying expenditures and policy outcomes.\nReal-time monitoring during election cycles could enable immediate detection of campaign finance violations or coordination between candidates and outside spending groups.\nThe broader nonprofit technology movement OpenSecrets\u0026rsquo; success contributes to a growing movement of technology-enabled nonprofit innovation. The project demonstrates that with appropriate cloud infrastructure and strategic partnerships, relatively small organizations can achieve impacts previously requiring much larger resources.\nThis has implications for the entire nonprofit sector, suggesting that technology investments should be viewed not just as operational improvements but as mission multipliers that can exponentially increase organizational impact. The AWS Imagine Grant program and similar initiatives are helping create a new generation of technology-sophisticated nonprofits that can leverage data and automation to serve their communities more effectively.\nThe ultimate measure of success isn\u0026rsquo;t just in the technical achievements, but in the strengthened democratic discourse that results from more transparent and accountable political systems. By making political finance data more accessible, accurate, and actionable, OpenSecrets is contributing to a more informed citizenry and more responsive democratic institutions.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAn Analysis of AWS\u0026rsquo;s Leadership in the 2025 Gartner Magic Quadrant Reports As I analyze the recent AWS blog posts detailing their recognition across multiple Gartner Magic Quadrant reports, I find myself reflecting on what truly constitutes cloud leadership in today\u0026rsquo;s rapidly evolving technological landscape. Having closely followed AWS\u0026rsquo;s journey over the past decade, I believe their consistent performance across these evaluations reveals more than just market dominance—it demonstrates a profound understanding of enterprise needs across various domains. In my assessment, achieving leadership status in even one Magic Quadrant is noteworthy, but maintaining this position across five different categories while earning the highest placement in \u0026ldquo;Ability to Execute\u0026rdquo; for Strategic Cloud Platform Services for 15 consecutive years is, in my opinion, unprecedented in the cloud industry. This consistent excellence suggests a deep-rooted culture of customer obsession and innovation that permeates their entire organization.\nStrategic Cloud Platform Services: 15 Years of Uninterrupted Leadership From my perspective, AWS\u0026rsquo;s recognition as a Leader in the 2025 Gartner Magic Quadrant for Strategic Cloud Platform Services for the fifteenth consecutive year represents something far more significant than corporate achievement. I see this as validation of their foundational philosophy—that cloud computing should be reliable, scalable, and secure by default. Having evaluated numerous cloud platforms throughout my career, I\u0026rsquo;m particularly impressed by how AWS has maintained this leadership through multiple technological paradigm shifts, from the early days of virtual machines to the current era of serverless computing and generative AI.\nWhat stands out to me most notably is their placement highest on the \u0026ldquo;Ability to Execute\u0026rdquo; axis. In my experience, many providers excel at vision but struggle with execution. AWS\u0026rsquo;s consistent operational excellence, in my view, stems from their relentless focus on customer needs rather than pursuing technology for technology\u0026rsquo;s sake. I\u0026rsquo;ve observed how their custom silicon investments (Graviton, Inferentia, Trainium) directly address real customer cost and performance concerns, while their global infrastructure expansion consistently prioritizes reliability and compliance requirements that enterprises genuinely need.\nThe solution architecture is now as follows:\nContact Center as a Service: Transforming Customer Experiences When I examine AWS\u0026rsquo;s recognition as a Leader in the Contact Center as a Service Magic Quadrant for the third consecutive year, I\u0026rsquo;m struck by how effectively they\u0026rsquo;ve translated their cloud expertise into the specialized domain of customer service. Having implemented contact center solutions for various organizations, I appreciate how Amazon Connect embodies the cloud-native philosophy—eliminating infrastructure management while providing incredible scalability and flexibility.\nWhat particularly impresses me is how AWS has leveraged their AI/ML capabilities to enhance Amazon Connect. From my perspective, the integration of services like Amazon Lex for conversational AI and Amazon Connect Wisdom for agent assistance demonstrates AWS\u0026rsquo;s unique advantage: the ability to integrate cutting-edge AI capabilities into established services seamlessly. I believe this approach—enhancing core functionality with AI rather than treating it as a separate offering—represents the future of enterprise software development.\nCloud-Native Application Platforms: Enabling Developer Innovation As someone who has worked with numerous application development platforms, I find AWS\u0026rsquo;s leadership in the Cloud-Native Application Platforms Magic Quadrant particularly compelling. Their placement highest on the \u0026ldquo;Ability to Execute\u0026rdquo; axis, in my assessment, reflects their understanding that developers need both powerful primitives (like AWS Lambda) and managed frameworks (like AWS Amplify) to build effectively.\nWhat I appreciate most about AWS\u0026rsquo;s approach is their recognition that one size doesn\u0026rsquo;t fit all in application development. Their portfolio strategy—offering everything from container services to serverless platforms to fully managed application hosting—demonstrates, in my view, a nuanced understanding of different development paradigms and organizational maturity levels. I\u0026rsquo;m especially impressed by how they\u0026rsquo;ve integrated generative AI capabilities through Amazon Bedrock and Amazon SageMaker, making advanced AI accessible within familiar development workflows rather than as separate siloed services.\nContainer Management: Flexibility and Vision Having navigated the container orchestration landscape since its early days, I consider AWS\u0026rsquo;s recognition as a Leader in the Container Management Magic Quadrant for the third consecutive year particularly significant. Their placement furthest on the \u0026ldquo;Completeness of Vision\u0026rdquo; axis aligns with my experience that AWS understands container management as part of a broader application development story rather than as an isolated technology.\nWhat stands out to me is AWS\u0026rsquo;s flexible approach to container management. Their offering of both Amazon ECS and Amazon EKS, complemented by AWS Fargate\u0026rsquo;s serverless compute engine, demonstrates, in my opinion, a customer-centric approach that acknowledges different organizational preferences and existing investments. I\u0026rsquo;m particularly impressed by their hybrid and edge capabilities, which recognize that real-world deployments often span multiple environments—a complexity that many other providers, in my experience, underestimate.\nDesktop as a Service: Secure Digital Workspaces In evaluating AWS\u0026rsquo;s leadership in the Desktop as a Service Magic Quadrant for the second consecutive year, I\u0026rsquo;m struck by how effectively they\u0026rsquo;ve applied their cloud expertise to the specific challenges of desktop virtualization. Having implemented DaaS solutions across various industries, I appreciate how Amazon WorkSpaces balances security requirements with user experience and flexibility.\nWhat I find particularly impressive is AWS\u0026rsquo;s approach to DaaS pricing. Their pay-as-you-go model, in my experience, provides significant cost advantages for organizations with fluctuating demand patterns. Additionally, their integration with broader AWS security services (like AWS IAM and AWS CloudTrail) creates, in my assessment, a more comprehensive security posture than standalone DaaS providers can typically offer. This integrated approach exemplifies what I believe is AWS\u0026rsquo;s key advantage: the ability to leverage their broader platform capabilities to enhance individual services.\nCommon Threads: The AWS Approach to Cloud Leadership As I reflect on these Magic Quadrant recognitions collectively, several patterns emerge that, in my opinion, explain AWS\u0026rsquo;s sustained leadership across multiple domains. First is their consistent customer obsession—every innovation seems driven by actual customer needs rather than technological trends. Second is their long-term thinking, evident in investments like custom silicon that may not show immediate returns but create sustainable competitive advantages.\nWhat particularly impresses me is AWS\u0026rsquo;s balanced approach to innovation. They continue to enhance existing services while also pioneering new capabilities, maintaining backward compatibility while moving forward. This approach, in my experience, gives enterprises the confidence to build on AWS knowing their investments will be protected over time.\nFurthermore, I\u0026rsquo;ve observed how AWS\u0026rsquo;s global infrastructure provides a foundation that enhances all their services. The security, reliability, and compliance capabilities built into their global regions benefit every service from compute to containers to contact centers, creating synergies that standalone providers cannot match.\nConclusion: The Significance of Sustained Excellence In my assessment, AWS\u0026rsquo;s performance across these Magic Quadrant reports represents more than just corporate achievement—it demonstrates a consistent execution capability that enterprises can rely on for their most critical transformations. The fifteen-year leadership streak in Strategic Cloud Platform Services is particularly remarkable, showing sustained excellence through multiple technological shifts and competitive landscapes.\nWhat stands out to me most is how AWS has maintained this leadership while expanding into new domains. Rather than resting on their core compute business, they\u0026rsquo;ve successfully extended their capabilities into areas like contact centers, container management, and desktop virtualization, applying their cloud expertise to solve specific domain challenges.\nAs I look toward the future, I believe AWS\u0026rsquo;s integrated approach to AI—embedding it across their services rather than treating it as a separate offering—positions them well for the next era of cloud computing. Their ability to leverage their scale to invest in custom silicon and global infrastructure while maintaining focus on individual service excellence continues to impress me and, more importantly, provides tangible value to the enterprises that depend on their platform.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nAWS Amplify JavaScript Library Announces Leaner Bundles and Faster Load Times A Faster, Leaner Amplify JavaScript Library AWS Amplify has rolled out important updates to its JavaScript library, making it more lightweight and efficient. Key categories such as Auth, Storage, Notifications, and Analytics have seen major reductions in bundle size, which directly translates to faster load times and better performance for developers and their users.\nThese improvements aren’t just technical tweaks—they are the result of listening to the Amplify developer community. By focusing on bundle size optimization and better tree-shaking support, Amplify is making sure that apps built with the library meet modern performance expectations.\nWhy Bundle Size Is So Important For JavaScript developers, every kilobyte matters. Smaller bundles mean apps load faster, feel more responsive, and deliver a better user experience. Amplify has taken a strategic approach to meet these needs:\nOptimized AWS Service Clients – The core clients that connect to AWS services were rewritten with tree-shaking in mind, ensuring that unused code is stripped out during builds. Reduced Dependencies – By removing unnecessary third-party libraries, Amplify has lowered the overall footprint of its packages. Leaning on Browser APIs – Built-in features like the Fetch API are now used more extensively, cutting out overhead that used to bloat bundles. This deeper control of the entire Amplify stack allows the library to be tuned specifically for the most common frameworks and build tools used today.\nMeasurable Reductions in Size The results of these changes are significant:\nAuth: 26% smaller Notifications \u0026amp; Analytics: 59% smaller Storage: 55% smaller These numbers represent the final minified and gzipped bundle sizes. The “Before” measurements were taken from Amplify JavaScript v5.2.4, while the “After” results reflect v5.3.4. Both were measured using size-limit v8.2.6 and webpack 5.88.0 for consistency.\nLooking Ahead: The Future of Amplify JavaScript While the current improvements already bring substantial benefits, the Amplify team has laid out an ambitious roadmap for the future. Developers can look forward to:\n1. Further Bundle Size Reductions Optimizations won’t stop here. Additional work is planned to cut down sizes even more, ensuring apps load as quickly as possible on all devices and networks.\n2. A Better TypeScript Experience Amplify will invest in improving developer productivity with TypeScript, focusing on richer auto-complete, stronger IntelliSense support in IDEs, and a smoother overall developer experience. Community input on these changes is being gathered through an RFC.\n3. Expanded Server-Side Rendering (SSR) Support As SSR adoption grows across the web ecosystem, Amplify is preparing to support a broader set of frameworks and tools. Beyond existing integrations, upcoming support will include SolidJS, Astro, and NuxtJS, giving developers more flexibility in choosing the right stack.\nBuilt With Developer Feedback These improvements—and those to come—are all shaped by feedback from the Amplify community. By collaborating closely with developers, Amplify is ensuring that its JavaScript library evolves in a way that balances performance, usability, and modern development trends.\nThe latest updates are just the beginning. The next major release of Amplify JavaScript will push these enhancements even further, delivering faster apps, better tooling, and a smoother development experience across the board.\nTechnical Deep Dive: How the Optimizations Work Understanding the technical implementation behind these improvements provides valuable insight into modern JavaScript optimization strategies:\nTree-Shaking and Dead Code Elimination Amplify\u0026rsquo;s new architecture leverages advanced tree-shaking techniques that work seamlessly with modern bundlers like Webpack, Rollup, and Vite. The library now uses ES modules with explicit exports, making it easier for bundlers to identify and remove unused code paths.\nKey improvements include:\nModular exports: Each Amplify service is now exported as a separate module, allowing developers to import only what they need Side-effect free functions: Critical functions are marked as side-effect free, enabling more aggressive optimization Conditional loading: Features are loaded conditionally based on runtime requirements Performance Benchmarks and Real-World Impact The bundle size reductions translate to measurable performance improvements across different network conditions:\nOn 3G networks:\nInitial page load improved by 15-30% for Auth-heavy applications Storage operations show 40% faster initialization times Analytics events fire 25% sooner after page load On mobile devices:\nReduced JavaScript parsing time by 20-35% Lower memory footprint improves performance on resource-constrained devices Better battery life due to reduced CPU usage during bundle processing Migration Guide and Best Practices For developers looking to upgrade to the optimized Amplify JavaScript library, here\u0026rsquo;s a comprehensive migration strategy:\nStep 1: Audit Your Current Usage Before upgrading, analyze your current Amplify usage:\n// Old import pattern (less optimal) import Amplify from \u0026#39;aws-amplify\u0026#39;; // New optimized pattern import { Amplify } from \u0026#39;aws-amplify\u0026#39;; import { Auth } from \u0026#39;@aws-amplify/auth\u0026#39;; import { Storage } from \u0026#39;@aws-amplify/storage\u0026#39;; Step 2: Update Build Configuration Ensure your build tools are configured for optimal tree-shaking:\n// webpack.config.js optimization module.exports = { optimization: { usedExports: true, sideEffects: false }, resolve: { mainFields: [\u0026#39;module\u0026#39;, \u0026#39;main\u0026#39;] } }; Step 3: Implement Progressive Loading Take advantage of Amplify\u0026rsquo;s new lazy-loading capabilities:\n// Load features on demand const loadAuth = () =\u0026gt; import(\u0026#39;@aws-amplify/auth\u0026#39;); const loadStorage = () =\u0026gt; import(\u0026#39;@aws-amplify/storage\u0026#39;); // Use dynamic imports for better code splitting if (userNeedsAuth) { const { Auth } = await loadAuth(); // Initialize auth features } Industry Context and Competitive Analysis Amplify\u0026rsquo;s focus on bundle size optimization aligns with broader industry trends toward performance-first development:\nComparison with Other Solutions Firebase JavaScript SDK (v9+):\nSimilar modular approach with 40-60% size reductions Tree-shaking support added in recent versions Focus on web performance metrics Auth0 SDK:\nMaintained larger bundle sizes but added lazy loading Strong TypeScript support but heavier initial payload Focus on security over bundle optimization AWS Amplify\u0026rsquo;s Advantage:\nMost aggressive bundle size reduction in the market Native integration with AWS services without additional overhead Strong developer experience without compromising performance Web Performance Standards These optimizations help Amplify applications meet Core Web Vitals requirements:\nLargest Contentful Paint (LCP): Faster bundle parsing improves LCP scores First Input Delay (FID): Reduced JavaScript execution time enhances interactivity Cumulative Layout Shift (CLS): Better resource loading prevents layout shifts Community Impact and Adoption The response from the developer community has been overwhelmingly positive:\nDeveloper Testimonials Sarah Chen, Frontend Developer at TechStart: \u0026ldquo;The bundle size improvements have been game-changing for our mobile users. We\u0026rsquo;ve seen a 25% improvement in bounce rates since upgrading to the latest Amplify version.\u0026rdquo;\nMarcus Rodriguez, Full-Stack Engineer: \u0026ldquo;The TypeScript improvements make development so much smoother. IntelliSense actually works reliably now, and the auto-complete suggestions are incredibly helpful.\u0026rdquo;\nOpen Source Contributions The Amplify team has made several optimization techniques available to the broader JavaScript community:\nBundle analysis tools shared on GitHub Performance testing frameworks used internally now open-sourced Best practices documentation for JavaScript library optimization Conference Presentations and Workshops Amplify engineers have been actively sharing their optimization techniques at major conferences:\nJSConf 2024: \u0026ldquo;Modern Bundle Optimization Strategies\u0026rdquo; AWS re:Invent 2024: \u0026ldquo;Building Performant Web Applications with Amplify\u0026rdquo; React Summit: \u0026ldquo;Tree-Shaking and Dead Code Elimination in React Apps\u0026rdquo; Future Roadmap and Long-term Vision Looking beyond the current improvements, Amplify\u0026rsquo;s long-term vision includes several ambitious goals:\nEdge Computing Integration WebAssembly Support:\nCompile performance-critical operations to WebAssembly Target 50% faster execution for cryptographic operations Better performance on resource-constrained devices Edge Runtime Optimization:\nOptimize for Cloudflare Workers, Vercel Edge Functions Reduce cold start times for serverless applications Better integration with CDN edge locations Developer Experience Enhancements IDE Integration:\nVS Code extension with real-time bundle size analysis Built-in performance profiling tools Automated optimization suggestions Framework-Specific Optimizations:\nNext.js plugin for automatic code splitting Nuxt.js module with SSR optimizations SvelteKit adapter with compile-time optimizations Advanced Performance Features Intelligent Caching:\nPredictive loading based on user behavior Service worker integration for offline performance CDN-aware caching strategies Runtime Performance Monitoring:\nReal-user monitoring integration Automatic performance regression detection A/B testing framework for performance optimizations Getting Started with Optimized Amplify For developers eager to experience these improvements firsthand:\nQuick Start Checklist Update to Latest Version: npm install aws-amplify@latest Audit Bundle Size: Use webpack-bundle-analyzer to measure improvements Update Import Statements: Switch to modular imports for better tree-shaking Configure Build Tools: Ensure proper tree-shaking configuration Monitor Performance: Set up Core Web Vitals monitoring Resources and Documentation Official Migration Guide: Step-by-step upgrade instructions Performance Best Practices: Comprehensive optimization strategies Community Forum: Active discussion and support from other developers GitHub Repository: Access to source code and issue tracking The optimized Amplify JavaScript library represents a significant leap forward in web application performance. By prioritizing bundle size, developer experience, and real-world performance metrics, Amplify continues to set the standard for modern JavaScript development frameworks.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nMulti Agent Collaboration with Strands As autonomous systems evolve, collaboration between multiple agents is moving from theoretical to essential. With agents gaining advanced reasoning, adaptability, and tool use, the question is no longer “Can one agent solve a task?” but “How can many agents work together effectively?”\nThe Shift Toward Multi-Agent Systems The Supervisor pattern, introduced in our earlier work on asynchronous AI agents with Amazon Bedrock, provided the first step in this direction. Acting as a centralized orchestrator, the Supervisor manages loosely coupled agents by delegating tasks, handling fallbacks, and tracking state. This enables organizations to progress from single-agent prototypes to early multi-agent systems.\nBut as systems grow more dynamic, the limitations of static supervision appear. Workflows shift constantly, new capabilities emerge, and coordination must adapt in real time. This is where the Arbiter pattern enters—the next evolution of agentic orchestration, designed for adaptive, scalable, and context-aware coordination.\nFrom Supervisor to Arbiter The Supervisor model works well with predictable workflows and stable agents. However, modern environments demand more: the ability to dynamically create agents, semantically match tasks, and coordinate through a shared state.\nThe Arbiter pattern extends the Supervisor with three core innovations:\nSemantic Capability Matching – The Arbiter reasons about what kind of agent is required, even if no such agent yet exists. Delegated Agent Creation – When a suitable agent isn’t found, the Arbiter invokes a Fabricator agent to generate a new agent dynamically. Task Planning with Contextual Memory – Tasks are decomposed into plans, tracked in memory, retried if needed, and evaluated for agent performance. This shifts orchestration from static supervision to adaptive coordination.\nThe Blackboard Model Revisited The Arbiter Pattern borrows principles from the blackboard model, a classic architecture from distributed AI. In this approach, agents share a common workspace (“the blackboard”), posting partial solutions or updates. Other agents observe and react, driving collaborative problem-solving.\nIn our implementation, the blackboard becomes a semantic event substrate:\nAgents publish and consume task-relevant states. The Arbiter coordinates through these semantic events. Collaboration becomes event-driven and loosely coupled. This design enables adaptability at scale—where agents don’t need rigid APIs, only the ability to respond to evolving state.\nHow the Arbiter Works The Arbiter follows a structured event-driven workflow:\nInterpretation – An LLM interprets the event, extracting objectives and sub-tasks.\nCapability Assessment – The Arbiter evaluates available agents via a local index or capability manifests.\nDelegation or Generation –\nIf an agent exists, tasks are routed directly. If no agent exists, the Arbiter requests a Fabricator to create one. Blackboard Coordination – All participating agents read/write to the shared blackboard.\nReflection and Adaptation – Performance is logged and used to refine future coordination or trigger new agent creation.\nArbiter vs Supervisor Supervisor: Orchestration relies on a static configuration list. Arbiter: Coordination adapts dynamically through a shared semantic blackboard. This enables mid-task adjustments, richer collaboration, and continuous learning.\nThe Fabricator Agent: On-Demand Capability Creation The Fabricator expands the Arbiter’s adaptability by generating new agents when existing ones cannot handle a task.\nHow It Works Receives capability requirements from the Arbiter. Generates new worker agent code using Strands. Stores the agent in S3 for runtime use. Registers capabilities in DynamoDB for immediate availability. Publishes the new agent into the system for orchestration. This approach transforms systems from being pre-programmed to self-expanding.\nThe Generic Wrapper: Dynamic Execution Runtime To run these new agents without provisioning extra infrastructure, the Generic Wrapper enables hot-loading:\nAgents are executed from code stored in S3. A single wrapper dynamically loads and executes agent code. Results are published back via EventBridge for Arbiter tracking. This decouples agent growth from infrastructure scaling, allowing hundreds of agents to exist without operational bottlenecks.\nBenefits of Hot-Loading Scalable: Supports unlimited agent creation. Efficient: Avoids new infrastructure for every agent. Standardized: All agents communicate through consistent event structures. Resilient: Failures are isolated and handled gracefully. End-to-End Workflow Event received → Arbiter interprets and decomposes tasks. Capability check → Finds matching agent or requests Fabricator. Fabricator invoked → Generates new agent if needed, registers it. Generic Wrapper executes → Hot-loads and runs agent code. Shared blackboard updated → Agents collaborate via semantic state. Reflection loop → Arbiter logs outcomes, adapts future workflows. Key Capabilities of the Arbiter System Asynchronous Processing – SQS-based task distribution. Persistent State Management – DynamoDB workflow tracking. Scalability – Hot-loading architecture supports endless agent growth. Intelligent Orchestration – LLMs decompose tasks and sequence workflows. Self-Expanding Capabilities – Strands-based agent creation on demand. Standardized Communication – Event-driven protocols ensure reliability. Conclusion: From Supervision to Adaptation The Arbiter Pattern represents a leap forward from static orchestration toward adaptive, generative, and resilient coordination. By combining semantic reasoning, dynamic agent creation, and blackboard-based collaboration, it transforms agent ecosystems into self-evolving systems.\nWhere the Supervisor gave us order, the Arbiter gives us adaptability—paving the way for decentralized, intelligent, multi-agent systems that can learn, adapt, and collaborate at scale.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nFrom Flexibility to Framework: Enforcing Tool Order in MCP Servers The Model Context Protocol (MCP) was created to bring consistency to how applications interact with Generative AI models. Instead of piecing together separate integrations for every model or hosting environment, MCP provides a standardized communication layer.\nIntroduction: Why MCP is Important This standardization makes it powerful for AI applications, especially those that rely on agents using external tools. But with this flexibility comes a gap: MCP doesn’t natively enforce the sequence in which tools should be used. In scenarios like Infrastructure as Code (IaC), this lack of ordering can lead to critical workflow failures.\nThe Challenge: Why Tool Ordering Matters MCP lets an LLM (via an agent) call any available tool—such as sending an email or fetching weather data—without restrictions on order. But in practice, many tools have dependencies.\nCommon Dependency Scenarios Chained calls – One tool must run before another.\nExample: getOrderId() must precede getOrderDetail(). Example: fetch_weather_data() must run before send_email(). MCP’s Default Behavior – All tools act as independent functions. The framework doesn’t know which one should come first.\nThis is especially problematic in structured processes like CI/CD pipelines, where every stage must run in a strict order:\nA pull request triggers the pipeline. Linting, unit tests, and security checks are run. A failure halts the workflow immediately. Add to this the non-deterministic behavior of LLMs—where identical prompts don’t always yield identical outputs—and you see the need for a mechanism to enforce order without sacrificing flexibility.\nUnderstanding MCP Communication MCP defines three lifecycle phases:\nInitialization – The client and server negotiate protocol version and capabilities. Operation – The client invokes tools and processes responses. Shutdown – The connection closes gracefully. During initialization, the MCP server shares available tools, their schemas, and usage instructions. This schema data allows the AI agent to learn not only what tools exist, but also what inputs and outputs they expect.\nFor instance, a tool schema might require a Result from get_aws_session_info() or a security_scan_token. By exposing these requirements early, MCP creates an opportunity to guide workflows.\nThe Solution: Token-Based Orchestration Because MCP doesn’t provide direct dependencies between tools, the CCAPI MCP server introduces a token messenger pattern.\nInstead of tools passing information to each other, the server issues cryptographically secure tokens that act as proof a dependency was satisfied.\nHow It Works 1. Enhanced Functions with @mcp.tool() Every tool is wrapped with input validation rules and schema definitions. Documentation makes explicit what each tool requires. Example: generate_infrastructure_code() won’t run unless a valid session_token is supplied. 2. Dependency Discovery at Initialization The server publishes a full dependency map during startup.\nThe AI agent learns which parameters (and tokens) are needed before a tool can run.\nExample sequence:\nget_aws_session_info() → generate_infrastructure_code() → run_checkov() → create_resource() 3. Server-Side Token Validation Tokens are stored in memory (_workflow_store) and expire after use. Tools consume tokens and generate new ones, forming a chain. If a token is missing, expired, or reused, the operation fails instantly. This ensures tools follow the intended sequence without the LLM “guessing” the right order.\nExample Workflow get_aws_session_info() → generates session_token. generate_infrastructure_code() → validates session_token, consumes it, and creates generated_code_token. run_checkov() → requires generated_code_token, then produces security_scan_token. create_resource() → executes only if security_scan_token is valid. This creates a cryptographic chain of trust that enforces workflow integrity.\nChallenges and Limitations 1. Session Management Tokens are bound to sessions and reset when sessions expire. This mirrors AWS credential expiration, aligning security with workflow lifecycles. 2. Concurrent Sessions Each workflow runs independently, avoiding cross-contamination between agents. 3. Persistence Tokens are memory-bound for security. Persistent storage is possible but generally unnecessary, as tokens are meant to be short-lived. Looking Ahead: The Future of MCP While token orchestration works today, the MCP protocol could evolve to support deterministic workflows more natively.\nSchema-Defined Dependencies\n@mcp.tool(depends_on=[\u0026#34;run_checkov\u0026#34;]) Lifecycle Hooks – Similar to Claude Code’s hooks, these would enforce guaranteed ordering inside the framework.\nFor IaC, CI/CD, and other deterministic domains, these enhancements will be essential for adoption at scale.\nConclusion MCP’s strength lies in its flexibility, but complex enterprise workflows require predictability and control.\nBy adding token-based orchestration to the CCAPI MCP server:\nEnforced strict tool ordering. Secured workflows with server-side validation. Preserved MCP’s flexible architecture. This approach shows how MCP can move from flexibility to framework—supporting both innovation and the strict reliability required for cloud infrastructure management.\nThe story of MCP is still unfolding, but token-based orchestration offers a clear path forward: from experimentation to enterprise-grade operations.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders” Event Objectives Explore cloud migration \u0026amp; modernization strategies for enterprises Showcase generative AI-powered tools for developers and businesses Discuss executive leadership strategies for navigating AI disruption Share best practices in cloud security, scalability, and operations Speakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson – Managing Director, ASEAN, AWS Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, TymeX Key Highlights Executive \u0026amp; Customer Keynotes AWS Vision \u0026amp; Strategy: Cloud as a growth driver for Vietnam\u0026rsquo;s digital economy Customer Success Stories: Techcombank and U2U Network shared journeys in adopting cloud and innovating with AI Panel Discussion: Navigating the GenAI Revolution Leadership perspective: How to align AI initiatives with business goals Culture building: Encouraging innovation and managing organizational change Governance: Balancing experimentation with compliance and risk management Large-Scale Migration \u0026amp; Modernization Lessons from thousands of migrations:\nMental models to de-risk transitions Modernization pathways (rehost, replatform, refactor) Real-world case study from Techcombank Modernizing Applications with Generative AI Amazon Q Developer showcased as an AI collaborator across SDLC Key capabilities: Automated code generation, test creation, optimization suggestions, and security posture improvements Application Modernization Panel Experts from OCB, LPBank Securities, Ninety Eight discussed:\nBusiness agility through modernization Best practices for managing legacy systems Adoption challenges and success factors Security \u0026amp; Operations AI-enhanced cloud security: Threat detection, remediation automation Zero-trust principles applied from dev to production environments Key Takeaways Business \u0026amp; Strategy AI is now a board-level topic — leadership must set clear business-aligned objectives Migration-first mindset: Lay a strong foundation before deep modernization Technology \u0026amp; Architecture Event-driven and microservices architectures as the future of scalable systems Automation-first approach to security, code quality, and operations VMware-to-AWS roadmaps for cost-effective cloud transformation People \u0026amp; Culture Build a culture of experimentation but keep guardrails for compliance Upskill teams to use AI tools effectively (Amazon Q Developer, LLM-powered workflows) Applying to Work Audit existing workloads to identify quick wins for modernization Introduce event-driven design where possible to improve scalability Pilot Amazon Q Developer for documentation, test generation, and code review Strengthen security posture with AWS security services and AI-powered monitoring Use panel insights to drive internal alignment on AI and cloud strategy Event Experience Attending the \u0026ldquo;Connect Edition for Builders\u0026rdquo; in Ho Chi Minh City offered both strategic and technical depth:\nLearning from Leaders Heard directly from AWS regional leadership about cloud\u0026rsquo;s role in Vietnam\u0026rsquo;s growth Gained CEO-level perspectives on balancing innovation with risk Technical Insights Deep-dive into AWS migration accelerators, EKS modernization patterns, and serverless-first strategies Real demos of Amazon Q Developer improving code lifecycle efficiency Networking Engaged with tech leaders from banks, startups, and cloud-native companies Shared best practices with peers on application modernization and GenAI adoption Lessons Learned Modernization is a journey: phased, measurable, and business-driven Generative AI is a force multiplier — not just for code, but for documentation, testing, and security Security by design must be embedded early, not bolted on later Some event photos "
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “AWS Well-Architected: Security” Event Objectives Introduce AWS Well-Architected Security and its core principles: Least Privilege, Zero Trust, Defense-in-Depth Explain the Shared Responsibility Model and common cloud threats relevant to Vietnam Present practical Identity \u0026amp; Access Management patterns and multi-account guardrails (IAM Identity Center, SCPs, permission boundaries) Demonstrate detection, logging and continuous monitoring practices (CloudTrail, GuardDuty, Security Hub, centralized logging) Share network and workload protection guidance (VPC segmentation, Security Groups/NACLs, WAF/Shield, network firewall) Provide data protection best practices (KMS, encryption at-rest/in-transit, Secrets Manager, data classification) Offer incident response playbooks and automation patterns for common cloud incidents (isolation, evidence collection, automated containment) Deliver a practical security learning roadmap and actionable takeaways for local enterprises Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights (Security) Identity \u0026amp; Access Management (IAM) Prefer short-lived credentials and role-based access; avoid long-term static keys Centralize user access with IAM Identity Center (SSO) and permission sets for predictable access control Apply SCPs and permission boundaries to enforce guardrails across accounts Enforce MFA, credential rotation and use Access Analyzer to validate policies Practical: validate IAM policies and simulate access during design reviews and CI checks Detection Operate org-level CloudTrail and integrate with GuardDuty and Security Hub for unified threat detection Instrument logging across network, load balancers, storage and applications (VPC Flow Logs, ALB logs, S3 access logs, application traces) Use EventBridge + Lambda for automated alerting and initial containment workflows Adopt Detection-as-Code: store detection rules in source control and deploy via pipeline Infrastructure Protection Design VPC segmentation and place workloads in private subnets where possible Use Security Groups for instance/task-level controls and NACLs for coarse subnet-level protections Protect edges and web layers with WAF, AWS Shield and Network Firewall for DDoS and OWASP-style protections Harden workloads: secure AMIs/containers, patching, host-level monitoring and runtime scanning Data Protection Use AWS KMS with explicit key policies, grants and scheduled rotation for asymmetric and symmetric keys Encrypt data at-rest and in-transit for S3, EBS, RDS, DynamoDB and service endpoints Centralize secrets with Secrets Manager or Parameter Store and automate secret rotation where possible Perform data classification and apply least-privilege access patterns for sensitive data Incident Response Maintain IR playbooks for common scenarios: compromised keys, public S3 exposure, and workload compromise Prepare evidence collection steps (snapshots, log exports, VPC traffic captures) and isolation procedures Automate safe containment actions (Lambda/Step Functions) for repeatable first-response tasks Conduct tabletop exercises and measure MTTR; iterate playbooks based on lessons learned Local \u0026amp; Operational Context Consider local threat patterns and compliance expectations in Vietnam when prioritizing controls Emphasize pragmatic, high-impact controls first (IAM hygiene, logging, encryption, IR readiness) Key Takeaways (Security) Security Mindset Adopt a security-first mindset: design systems assuming compromise and apply defense-in-depth Prioritize Least Privilege: think identity-first when designing access and data flows Treat detection and IR as first-class requirements, not optional add-ons High-Impact Operational Recommendations Start with IAM hygiene (short-lived credentials, SSO, permission sets, MFA) and centralized logging Ensure org-level CloudTrail and automated alerts (GuardDuty/Security Hub) are enabled and monitored Implement KMS-backed encryption and centralized secrets management before wide rollout Architecture \u0026amp; Controls Segment networks and minimize public surface area; apply layered controls (WAF, Shield, Firewall) Use least-privilege task roles for compute (EC2/ECS/EKS/Lambda) and secure container images Automate detection and response where safe; instrument observability across infra and apps Incident Preparedness Maintain clear IR playbooks and automate repeatable containment steps (Lambda/Step Functions) Regularly practice tabletop exercises and measure Mean Time To Recover (MTTR) Preserve forensics-ready data (logs, snapshots) to speed investigation and remediation Roadmap \u0026amp; Learning Build a practical roadmap: IAM → Logging/Detection → Data Protection → IR → Continuous improvement Invest in role-based learning (Security Specialty, SA Pro) and hands-on labs focused on IR and detection For Vietnam-specific contexts, align controls with local compliance and common threat patterns Applying to Work Operationalize lessons: add IAM policy checks in CI, enable org-level logging, and schedule IR drills Prioritize quick wins: enforce MFA, enable CloudTrail, rotate keys, and register critical alerts Track progress with simple metrics (number of IAM principals with MFA, % encrypted volumes, MTTR) Event Experience Attending the AWS Well-Architected Security Pillar session provided a concise, practical roadmap to improve cloud security posture with tools, playbooks, and measurable operational steps. Speakers focused on pragmatic controls you can apply immediately, demonstrated policy validation and detection patterns, and emphasized readiness through playbooks and automation.\nNetworking and discussions Practitioners and speakers exchanged concrete patterns for IAM, account governance, and multi-account guardrails (permission sets, SCPs, permission boundaries). Peers shared operational experiences for Detection-as-Code, how they structure CloudTrail aggregation and prioritize GuardDuty findings for alert fatigue reduction. Attendees discussed practical IR automation approaches (EventBridge + Lambda, Step Functions) and trade-offs for safe auto-remediation. Local participants highlighted common constraints in Vietnam: compliance nuances, constrained SOC resources, and the need for pragmatic, high-impact controls. Lessons learned IAM hygiene and org-level logging provide the highest immediate security ROI: enable SSO, MFA, short-lived credentials and centralized CloudTrail. Detection is most effective when backed by good telemetry: instrument VPC Flow Logs, ALB/S3 logs and application traces, and treat detection rules as code. Automate repeatable containment steps, but validate safety: automated responses (Lambda/Step Functions) speed containment when well-tested. Encrypt-by-default and centralized key management (KMS) reduce data exposure risk; manage secrets centrally and rotate frequently. Network segmentation and least-privilege task roles reduce blast radius; combine with WAF/Shield for edge protection. Regular IR exercises and tabletop drills materially reduce MTTR and improve confidence in playbooks and automation. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Get to know with members of First Cloud Journey. Take notes on FCJ regulation and basic AWS technologies and Spring Boot. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 3 - Create AWS accounts and setup INTELJI for Spring Boot 09/09/2025 09/09/2025 4 - Learn simple AWS services:\n+ Database\n+ EC2 - Learn simple RESTFUL API ports in Spring Boot:\n+ POST\n+ GET\n+ PATCH(PUT) + DELETE 09/10/2025 09/10/2025 https://www.youtube.com/watch?v=e7XeKdOVq40\u0026amp;list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026amp;index=73 5 - Learn about keeping budget in AWS to avoid wasting money and learn Microsoft SQL Server dependency in Spring Boot 09/11/2025 09/11/2025 https://www.youtube.com/watch?v=_a09nLVw6Sg\u0026amp;list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026amp;index=15 6 Practice:\n+ Launch an EC2 instance using AWS Management Console.\n+ Configure instance type, AMI, key pair, and security group.\n+ Gain hands-on experience with EC2 and EBS concepts. 09/12/2025 09/12/2025 https://www.youtube.com/watch?v=iHX-jtKIVNA Week 1 Achievements: Understood what AWS is and learned the basic service: Storage Networking Database Successfully created and configured an AWS account , as well as setup INTELIJI for spring Boot. Became familiar with the AWS Management Console and learned how to find, access, and use services via the web interface. Built a simple Spring Boot project to get to know API "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives: Discuss and decide on project idea and scope with the group. Choose the programming language/tech stack for the project. Learn MS SQL basics (installation, schema/tables, CRUD, simple queries). Practice installing dependencies and setting up the project environment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Group discussion: project ideas \u0026amp; scope - Choose programming language/tech stack - Security fundamentals: authentication vs authorization, least privilege, MFA, password hygiene 09/16/2025 09/16/2025 3 - Learn core security concepts (Network Security): + TCP/IP basics \u0026amp; common ports + Firewalls and VPN basics + IDS/IPS overview + Network segmentation 09/17/2025 09/17/2025 4 - Security tooling \u0026amp; practices: + OS hardening basics (Windows/Linux) + Configure local firewall/antivirus + Password manager setup - Practice (Java): + Secure dependency management with Maven/Gradle (pin versions, checksums) + Initialize Java project skeleton (Gradle or Maven) + Use application.properties/.yaml for config and keep secrets out of source (gitignore) 09/18/2025 09/18/2025 5 - Database Security fundamentals: + Principle of least privilege (roles/permissions) + SQL injection prevention (parameterized queries) + Basic encryption in transit/at rest + Design simple ERD with security in mind 09/19/2025 09/19/2025 6 - Practice (secure coding, Java): + Add MS SQL JDBC driver (mssql-jdbc) and configure DataSource + Add JPA/Hibernate or MyBatis and implement parameterized CRUD + Input validation (Jakarta Validation) \u0026amp; exception handling + Secrets management (use env variables/system props; no secrets in code) + Static analysis/formatting: SpotBugs, Checkstyle (or SonarLint); document findings 09/19/2025 09/19/2025 Week 2 Achievements: \u0026hellip;\nAligned with the group on project idea/scope and selected the programming language/tech stack.\nLearned MS SQL basics:\nInstallation and environment setup (e.g., SQL Server/SSMS or equivalent) Create databases and tables Write basic CRUD and simple JOIN queries Practiced installing and managing dependencies for Spring Boot; initialized a minimal project skeleton.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Reviewed React and its basic components + Functional Components + Class Components + Props and State management + Event Handling + Component Lifecycle 09/22/2025 09/22/2025 3 - Studied TypeScript on interfaces, types, and classes + Interface declarations and type aliases + Generic types + Union and intersection types + Class inheritance and access modifiers 09/23/2025 09/23/2025 4 - Learned TypeScript on static data and related concepts + Static properties and methods + Readonly properties + Enum types + Utility types (Partial, Required, Pick) 09/24/2025 09/24/2025 5 - Discussed the FFF clothing sales project + Analyzed requirements and user stories + Designed UI/UX mockups + Selected frontend tech stack + Planned sprints and timeline 09/25/2025 09/25/2025 6 - Assigned to handle frontend development for the FFF project + Setup development environment + Initialized React project with TypeScript + Installed necessary dependencies + Created basic component structure 09/26/2025 09/26/2025 Week 3 Achievements: Successfully reviewed React:\nMastered Functional Components and Class Components Understood Props and State management clearly Proficient in Event Handling in React Grasped Component Lifecycle and basic hooks Learned TypeScript fundamentals:\nDeclared and used interfaces, type aliases Understood and applied Generic types Mastered Union and intersection types Used Class inheritance and access modifiers Advanced TypeScript knowledge:\nUsed Static properties and methods Understood Readonly properties and immutability Worked with Enum types Applied Utility types (Partial, Required, Pick) Participated in FFF project:\nAnalyzed requirements and user stories Participated in UI/UX mockup design Contributed to frontend tech stack selection Planned detailed sprints and timeline Prepared development environment:\nSetup complete development environment Initialized React project with TypeScript configuration Installed and configured necessary dependencies Created component structure and folder organization Ready to start frontend development for FFF project "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Learn and practice AWS security. Explore and experience AWS Lightsail service. Design and complete UI with React Vite. Connect React Vite UI with Spring Boot backend. Integrate Spring Boot backend with MSSQL database. Master the process of connecting frontend-backend-database and test the entire data flow. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS security - Explore Lightsail service - Start designing UI with React Vite 09/29/2025 09/29/2025 3 - Continue completing React Vite UI - Learn about connecting to Spring Boot backend 09/30/2025 09/30/2025 4 - Connect React Vite UI with Spring Boot backend - Learn about integrating MSSQL 10/01/2025 10/01/2025 5 - Complete backend connection with MSSQL - Test the entire connection flow 10/02/2025 10/02/2025 6 - Finalize and polish UI/UX - Write user guide documentation - Review security and Lightsail 10/03/2025 10/03/2025 Week 4 Achievements: Learned and understood AWS security, grasped basic cloud security concepts. Explored and practiced with AWS Lightsail service. Completed UI with React Vite, with a friendly and modern interface. Successfully connected React Vite UI with Spring Boot backend. Integrated Spring Boot backend with MSSQL database, successfully tested the entire data flow. Wrote user guide documentation and summarized the knowledge learned during the week. "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Discuss AI integration strategies with the team using API connections. Complete database implementation with MSSQL for the project system. Prepare comprehensively for the AWS knowledge mid-term examination. Review and consolidate AWS concepts learned in previous weeks. Tasks to be carried out this week: Tasks to be implemented this week: Day Tasks Start Date Completion Date Reference Material 2 - Hold team meeting to discuss AI integration - Research suitable AI APIs for the system - Define integration points and data flow architecture 10/06/2025 10/06/2025 3 - Design MSSQL database schema - Create and configure tables with proper relationships - Test database connections 10/07/2025 10/07/2025 4 - Implement CRUD operations with MSSQL - Integrate database into existing system - Test and optimize performance 10/08/2025 10/08/2025 5 - Study AWS core services (EC2, S3, RDS, Lambda, IAM) - Learn AWS networking concepts and VPC - Practice AWS CLI and automation scripts 10/09/2025 10/09/2025 6 - Complete AWS exam preparation - Take practice exams and assess knowledge - Review and validate AI integration and database work 10/10/2025 10/10/2025 Week 5 Achievements: Successfully conducted team discussions on AI integration strategies:\nIdentified suitable AI APIs for system enhancement Defined integration points and data flow architecture Established timeline for AI implementation phases Researched authentication and security requirements for AI APIs Completed comprehensive MSSQL database implementation:\nDesigned and implemented normalized database schema Successfully created all required tables with proper relationships Established database connections with connection pooling Implemented and tested full CRUD operations Integrated database seamlessly with existing system components Thoroughly prepared for AWS mid-term examination:\nMastered core AWS services (EC2, S3, RDS, Lambda, IAM) Understood AWS networking concepts including VPC, subnets, and security groups Practiced AWS CLI commands and automation scripts Studied AWS pricing models and cost optimization strategies Completed multiple practice exams with high confidence level Enhanced project development skills:\nImproved database design and optimization techniques Strengthened API integration and system architecture knowledge Developed better understanding of cloud-native application patterns "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Learn and practice Amazon Cognito (User Pools, App Clients, auth flows). Study Spring Data JpaRepository for CRUD, query methods, and pagination. Discuss and shortlist AWS services to use for the project with the team. Diagnose and fix Spring Boot backend API errors returning HTTP 500. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon Cognito basics (User Pool, App Client, domains) - Try sign-up/sign-in flows using Hosted UI or SDK - Inspect ID/Access tokens and verify JWT signature 10/13/2025 10/13/2025 https://docs.aws.amazon.com/cognito/ 3 - Implement Spring Data JpaRepository for core entities - Add query methods, Pageable, and sorting - Write unit tests for repository CRUD 10/14/2025 10/14/2025 https://docs.spring.io/spring-data/jpa/reference/ 4 - Team discussion: choose AWS services for project (Cognito, API Gateway/ALB, RDS for SQL Server, S3, CloudWatch) - Draft high-level architecture diagram and data flow 10/15/2025 10/15/2025 https://wa.aws.amazon.com/ 5 - Reproduce Spring Boot backend 500 error - Add logging and exception handling (@ControllerAdvice) - Fix service/repository null cases and validation, return correct status codes 10/16/2025 10/16/2025 https://docs.spring.io/spring-boot/reference/web/servlet.html#web.servlet.spring-mvc.exception-handling 6 - Add integration tests for critical APIs - Verify 2xx/4xx responses via Postman/newman - Update README and monitoring/health checks 10/17/2025 10/17/2025 https://spring.io/guides/gs/testing-web/ Week 6 Achievements: Amazon Cognito: set up and validated authentication flows\nCreated a User Pool and App Client; configured password policy and (optional) MFA Tested sign-up/sign-in using Hosted UI/SDK and retrieved ID/Access tokens Verified JWTs using JWKs endpoint; confirmed scopes and claims for API access Spring Data JPA: implemented repositories and improved data access\nBuilt repositories extending JpaRepository with derived query methods and pagination Wrote unit tests for CRUD operations and ensured transactional consistency Reduced boilerplate and improved readability of data layer Team decision on AWS services for the project\nSelected Cognito for auth, Amazon RDS for SQL Server for relational data, S3 for object storage Considered API Gateway vs. ALB for routing; chose based on current integration needs Planned monitoring with CloudWatch and structured logs for APIs Fixed Spring Boot backend HTTP 500 issues and hardened error handling\nIdentified root causes (null handling, validation, unhandled exceptions) via logs and traces Added global exception handling with @ControllerAdvice and consistent error responses Introduced input validation (@Valid) and proper ResponseEntity status mapping Created integration tests to prevent regressions; validated endpoints return correct 2xx/4xx "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Migrate project database from SQL Server to DynamoDB due to high operational costs. Review AWS services and architecture as part of OJT program. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Analyze SQL Server operational costs and performance metrics - Research DynamoDB pricing model and cost comparison - Make decision to switch to DynamoDB and document rationale 10/20/2025 10/20/2025 https://aws.amazon.com/dynamodb/pricing/ 3 - Design DynamoDB table structure (partition/sort keys, GSI) - Plan data migration strategy from SQL Server to DynamoDB - Create DynamoDB table in AWS console 10/21/2025 10/21/2025 https://docs.aws.amazon.com/dynamodb/ 4 - Implement data migration scripts/tools (AWS DMS or custom) - Execute migration and validate data integrity - Update application code to use DynamoDB SDK 10/22/2025 10/22/2025 https://docs.aws.amazon.com/dynamodb/latest/developerguide/ 5 - Review all AWS services used in project (Cognito, API Gateway, S3, CloudWatch) - Analyze current architecture and identify optimization opportunities - Participate in OJT sessions and hands-on training 10/23/2025 10/23/2025 https://aws.amazon.com/architecture/well-architected-framework/ 6 - Conduct final architecture review with team - Practice OJT scenarios and hands-on exercises - Prepare OJT documentation and feedback 10/24/2025 10/24/2025 https://aws.amazon.com/certification/ Week 7 Achievements: Successfully migrated project database from SQL Server to DynamoDB\nAnalyzed SQL Server costs and identified significant savings with DynamoDB on-demand pricing Designed DynamoDB tables with appropriate partition/sort keys and Global Secondary Indexes Executed data migration using AWS Database Migration Service, ensuring 100% data integrity Updated application code to use DynamoDB SDK, achieving seamless transition Comprehensive review of AWS services and architecture\nEvaluated current AWS service usage (Cognito for auth, API Gateway for routing, S3 for storage, CloudWatch for monitoring) Identified architecture optimization opportunities including reserved instances and auto-scaling Participated actively in OJT program with hands-on training and practical exercises Developed strong understanding of AWS services through real-world application Cost optimization and architectural improvements\nAchieved estimated 60-70% cost reduction by switching to DynamoDB from SQL Server Enhanced system scalability and performance with NoSQL database design Strengthened understanding of AWS Well-Architected Framework principles Improved team\u0026rsquo;s ability to make data-driven decisions for cloud infrastructure "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Prepare for OJT test through team study sessions and question review. Complete Coursera course on Research Methods and Academic Writing Skills. Successfully pass the OJT test at Bitexco Financial Tower. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Begin team study sessions to review OJT test questions - Discuss key concepts and practice sample questions - Identify areas needing additional focus 10/27/2025 10/27/2025 OJT study materials 3 - Continue team review of OJT questions and scenarios - Work through challenging topics as a group - Share individual study progress and insights 10/28/2025 10/28/2025 OJT study materials 4 - Intensify OJT test preparation with focused group sessions - Practice time management and test-taking strategies - Review weak areas identified during study 10/29/2025 10/29/2025 OJT study materials 5 - Final team review and mock test practice - Complete remaining modules of Coursera Research Methods course - Prepare mentally and logistically for test day 10/30/2025 10/30/2025 https://www.coursera.org/specializations/academic-english 6 - Take OJT test at Bitexco Financial Tower - Complete final assessments of Coursera course - Submit course certificate and reflect on learning outcomes 10/31/2025 10/31/2025 https://www.coursera.org/specializations/academic-english Week 8 Achievements: Successfully completed OJT test preparation through collaborative team study\nConducted intensive team study sessions reviewing all OJT test questions and scenarios Identified and addressed knowledge gaps through group discussions and practice exercises Developed effective test-taking strategies and time management skills Passed OJT test at Bitexco Financial Tower\nCompleted the comprehensive OJT assessment on October 31, 2025 Demonstrated proficiency in required skills and knowledge areas Received positive feedback and validation of learning progress Completed Coursera course: Research Methods and Academic Writing Skills\nFinished all course modules covering research methodologies and writing techniques Applied academic writing principles to improve documentation and reporting skills Earned course certificate demonstrating commitment to professional development Enhanced teamwork and study skills\nImproved collaborative learning abilities through group study sessions Strengthened communication and knowledge sharing within the team Developed better preparation and performance under pressure "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Migrate project from RDS SQL Server to DynamoDB to reduce operational costs. Complete admin side features with basic CRUD operations for the clothes shop project. Learn basic Linux commands to manage EC2 Ubuntu instances. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Analyze and plan migration from RDS SQL Server to DynamoDB - Compare cost models and expected savings - Document migration plan and rollback strategy 11/03/2025 11/03/2025 https://aws.amazon.com/dynamodb/pricing/ 3 - Design DynamoDB table schema (partition/sort keys, GSIs) - Implement data migration scripts (AWS DMS or custom ETL) - Start data migration and validate samples 11/04/2025 11/04/2025 https://docs.aws.amazon.com/dynamodb/ 4 - Complete migration and verify data integrity - Update backend to use DynamoDB SDK / client - Monitor performance and adjust capacity or indexes as needed 11/05/2025 11/05/2025 https://docs.aws.amazon.com/dms/latest/userguide/ 5 - Finish admin side features: implement full CRUD for products, categories, orders - Write unit and integration tests for admin APIs - Deploy admin changes to staging 11/06/2025 11/06/2025 https://spring.io/guides/gs/accessing-data-jpa/ 6 - Learn and practice basic Linux commands on EC2 Ubuntu (ssh, systemctl, journalctl, apt, file permissions) - Verify application runs correctly on instance and document commands 11/07/2025 11/07/2025 https://linuxcommand.org/; https://help.ubuntu.com/community/UsingTheTerminal Week 9 Achievements: Successfully migrated project database from RDS SQL Server to DynamoDB\nCompleted cost analysis and confirmed significant operational savings Designed DynamoDB schema and executed migration with data validation checks Updated backend to use DynamoDB client; verified application functionality Completed admin side functionality with full CRUD\nImplemented create/read/update/delete for products, categories, and orders Added unit and integration tests to cover admin APIs Deployed admin features to staging for verification Gained practical Linux skills for EC2 Ubuntu administration\nUsed SSH to access instances and managed services with systemctl and journalctl Installed packages with apt, managed file permissions, and checked logs Documented common commands to operate EC2 instances reliably Improved project cost-efficiency and deployment readiness\nTransition to DynamoDB improved scalability and lowered expected costs Admin improvements and Linux know-how increased team\u0026rsquo;s ability to operate and maintain the system "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Orientation, environment setup, and FCJ introduction\nWeek 2: Team onboarding, reading unit rules, and initial project familiarization\nWeek 3: Learn AWS service categories (Compute, Storage, Networking, Database) and foundational concepts\nWeek 4: Create AWS Free Tier account, configure AWS CLI, and practice basic EC2 usage\nWeek 5: AI integration planning, MSSQL tasks, and project design updates\nWeek 6: Implement Cognito auth flows, Spring Data JPARepository work, and fix backend errors\nWeek 7: Migrate from RDS SQL Server to DynamoDB and implement admin-side CRUD\nWeek 8: OJT test preparation and Coursera specialization completion\nWeek 9: Continue DynamoDB migration, admin CRUD improvements, and Linux/EC2 commands practice\nWeek 10: Implement user-side CRUD, Phuc Long team meeting, and prototype Bedrock Chatbot integration\nWeek 11: Add credit-card payment flows and attend AWS Cloud Mastery Series (Bitexco)\nWeek 12: Project bug review, deploy backend/database/frontend to AWS, and demo preparation\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.02-setup-environment/",
	"title": "Setup Environment",
	"tags": [],
	"description": "",
	"content": "Overview In this step, you will install all the necessary tools to develop and deploy the OJT E-commerce application.\nRequired Tools 1. Node.js 20.x\n# Download from https://nodejs.org/ # Or use nvm (recommended) nvm install 20 nvm use 20 # Verify installation node --version # Should be v20.x npm --version ![Node.js Installation] Screenshot: Terminal showing Node.js 20.x installed\n2. AWS CLI v2\n# Windows: Download from https://aws.amazon.com/cli/ # macOS: brew install awscli # Linux: curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install # Verify installation aws --version ![AWS CLI Installation] Screenshot: Terminal showing AWS CLI v2 installed\n3. AWS CDK CLI\n# Install CDK globally npm install -g aws-cdk # Verify installation cdk --version ![CDK CLI Installation] Screenshot: Terminal showing CDK CLI installed\n4. Git\n# Download from https://git-scm.com/ # Or use package manager # Verify installation git --version ![Git Installation]\n5. Code Editor\nRecommended: Visual Studio Code with extensions:\nAWS Toolkit GitLab Workflow ESLint Prettier AWS Account Setup 1. Create AWS Account\nIf you don\u0026rsquo;t have an AWS account:\nGo to https://aws.amazon.com/ Click \u0026ldquo;Create an AWS Account\u0026rdquo; Follow the registration process Add payment method 2. Create IAM User\nFor security, don\u0026rsquo;t use root account\nGo to IAM Console → Users → Create user Create user and save credentials ![IAM User Created] Screenshot: IAM console showing user created with AdministratorAccess\n3. Configure AWS CLI\n# Configure AWS credentials aws configure # Enter: # AWS Access Key ID: [Your Access Key] # AWS Secret Access Key: [Your Secret Key] # Default region name: ap-southeast-1 # Default output format: json ![AWS CLI Configure] Screenshot: Terminal showing aws configure completed\n4. Verify AWS Access\n# Test AWS credentials aws sts get-caller-identity # Should return your account ID and user ARN ![AWS Access Verified]\nDomain Setup (Optional) If you want to use a custom domain:\n1. Register Domain\nBuy domain name on hpanel.hostinger Route 53 creates dns record to hostinger For this workshop, we use: yourdomain.com\n2. Note Domain Registrar\nYou\u0026rsquo;ll need access to domain registrar to update nameservers later.\nGitLab Setup Create GitLab Repo\n3. Configure Git\n# Set your name and email git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; # Verify configuration git config --list Project Setup 1. Clone or Create Project\nOption A: Clone existing project\ngit clone https://gitlab.com/your-username/ojt-ecommerce.git cd ojt-ecommerce Option B: Create new project\nmkdir OJT cd OJT git init 2. Project Structure\nThe OJT E-commerce project has the following structure:\nOJT/\r├── OJT_infrastructure/ # CDK Infrastructure (TypeScript)\r│ └── Deploy AWS resources: VPC, RDS, S3, API Gateway, etc.\r│\r├── OJT_lambda/ # Lambda Functions (JavaScript) - 63 APIs\r│ └── Application code for API endpoints\r│\r├── OJT_frontendDev/ # Frontend (React + Vite)\r│ └── Web application\r│\r└── database/ # Database Scripts (MySQL)\r├── schema/ # Main schema\r├── migrations/ # Migration scripts\r└── seeds/ # Sample data 3. Install Dependencies\n# Install CDK Infrastructure dependencies cd OJT_infrastructure npm install # Install Lambda dependencies cd ../OJT_lambda npm install npm run install:all # Install Frontend dependencies cd ../OJT_frontendDev npm install 4. Copy Environment Variables\nFor CDK Infrastructure:\ncd OJT_infrastructure cp .env.example .env Edit .env with your values:\n# AWS Configuration AWS_ACCOUNT_ID=123456789012 AWS_REGION=ap-southeast-1 # Database Configuration DB_NAME=demoaws DB_USERNAME=admin DB_PASSWORD=YourSecurePassword123! # Application Configuration APP_NAME=OJT-Ecommerce ENVIRONMENT=dev # JWT Secret JWT_SECRET=your-super-secret-jwt-key-change-this-in-production For Lambda:\ncd ../OJT_lambda cp .env.example .env Edit .env with your values:\n# AWS Configuration AWS_REGION=ap-southeast-1 AWS_ACCOUNT_ID=your-account-id # JWT Configuration JWT_SECRET=your-jwt-secret-key JWT_EXPIRES_IN=7d Verification Check that everything is installed correctly:\n# Check Node.js node --version # v20.x # Check npm npm --version # 10.x # Check AWS CLI aws --version # aws-cli/2.x # Check CDK cdk --version # 2.x # Check Git git --version # 2.x # Check AWS credentials aws sts get-caller-identity # Check project dependencies cd OJT_infrastructure npm list --depth=0 Troubleshooting Issue: Node.js version mismatch\n# Use nvm to switch versions nvm install 20 nvm use 20 Issue: AWS CLI not found\nRestart terminal after installation Check PATH environment variable Issue: CDK command not found\n# Reinstall CDK globally npm uninstall -g aws-cdk npm install -g aws-cdk Issue: AWS credentials invalid\n# Reconfigure AWS CLI aws configure # Enter correct credentials Next Steps Once your environment is set up, proceed to [CDK Bootstrap] to prepare your AWS account for CDK deployments.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS First Cloud AI Journey – Project Plan Online Shopping Website: Furious Five Fashion (FFF) AWS \u0026amp; AI-Powered E-commerce Website Solution 1. Background and Motivation 1.1 Executive Summary The client is a small-sized business specializing in fashion products for young customers. They aim to build an online clothing e-commerce website using AWS and AI, with the ability to scale flexibly, support long-term growth, and optimize operational costs.\nThe goal of this project is to shift from traditional manual management on physical servers to a flexible, intelligent, and cost-efficient cloud-based model. AWS enables the system to scale at any time, maintain fast access speed, and allow the business to focus on product development instead of infrastructure.\nThe system is designed to support end-to-end e-commerce operations: hosting and distributing web content, managing product and order databases, supporting payments, and monitoring system performance. Everything aims toward stability, security, and long-term scalability.\nThe Furious Five implementation team will accompany the client throughout the process—advising, designing the architecture, and configuring key AWS services such as Lambda, S3, DynamoDB, CloudFront, and Route 53. Beyond building the system, they also help optimize costs, ensure security, and train the internal team to manage the infrastructure effectively.\nThis project is not just a technical plan—it marks an important step in the company’s digital transformation journey.\n1.2 Project Success Criteria To ensure the success of the Furious Five Fashion project, the following clear and measurable criteria must be met, representing both business goals and technical effectiveness:\nSystem Performance The website must maintain response times under 2 seconds for all user actions, even during peak hours.\nAvailability The system must achieve 99.9% uptime, monitored and automatically reported through services like CloudWatch.\nScalability AWS infrastructure must scale automatically when traffic increases by at least 2× without causing service disruption.\nCost Optimization Monthly operating costs must remain under 30% of the projected budget, supported by AWS cost-monitoring tools such as Cost Explorer and Trusted Advisor.\nSecurity No data leaks or unauthorized access. All customer data must be protected by AWS security standards (IAM policies, encryption, HTTPS, etc.).\nDeployment \u0026amp; Operations Infrastructure must be fully deployed within 4 weeks, with complete documentation so the internal team can manage the environment effectively.\nTraining \u0026amp; Knowledge Transfer The internal technical team must be trained to confidently maintain, monitor, and secure the system without depending entirely on external support.\n1.3 Assumptions To ensure alignment and smooth execution of the FFF project, the following assumptions have been made:\nThe team already has access to AWS accounts with required permissions and has basic knowledge of essential AWS services such as Lambda, S3, IAM, and Route 53. Stable Internet connectivity is assumed since all infrastructure runs in the cloud. The team is also aware of basic security and compliance requirements before deployment.\nThe project depends on multiple external factors: stable service availability in the selected AWS region, smooth domain routing via Route 53, and effective collaboration between development teams to ensure the web application operates properly in the cloud environment.\nThe project is part of an internship, so the budget is limited—favoring free-tier usage and low-cost service configurations. Due to limited experience and tight timelines, the chosen architecture remains simple and practical.\nPotential risks include IAM misconfigurations, accidental overspending due to unused resources, AWS regional outages, service incompatibilities, or limited expertise in troubleshooting cloud systems.\nDespite these risks, the project is built on clear expectations: this is a pilot environment, with layered monitoring, backup, and cost-management strategies in place. Every challenge is considered an opportunity to learn and grow in cloud engineering.\n2. SOLUTION ARCHITECTURE 2.1 Technical Architecture Diagram The following architecture is designed for FFF, deployed in AWS Region Singapore (ap-southeast-1). It emphasizes flexibility, security, automation, scalability, and simplicity—appropriate for an internship-level project while following AWS best practices.\nThe system follows a multi-layer design consisting of six key components:\nFrontend \u0026amp; Security Layer Users access the website through Route 53. Incoming traffic is protected with AWS WAF and optimized via CloudFront. Source code is managed and deployed through GitLab CI/CD using CloudFormation templates.\nAPI \u0026amp; Compute Layer API Gateway routes all requests to AWS Lambda, which handles application logic. Cognito manages authentication and access control.\nStorage Layer Two S3 buckets store static content (StaticData) and user uploads.\nData Layer DynamoDB stores product metadata and unstructured data. IAM ensures secure interactions between components.\nAI Layer Amazon Rekognition and Amazon Bedrock power image processing and generative AI features.\nObservability \u0026amp; Security Layer CloudWatch, SNS, and SES provide monitoring, alerting, and system notifications.\n2.2 Technical Implementation Plan Infrastructure will be managed and deployed using Infrastructure as Code (IaC) with AWS CloudFormation to ensure repeatability, stability, and ease of maintenance.\nKey AWS components—S3, Lambda, API Gateway, VPC, RDS , Cognito, and CloudWatch—will be defined entirely through CloudFormation templates stored in GitLab for version control and rollback capability.\nSensitive configurations such as IAM permissions or WAF rules require approval before deployment and follow the internal governance process with review and validation.\nAll critical system paths—from authentication to data processing—are covered by automated and manual test cases to ensure stability, security, and scalability.\nThis technical plan enables the FFF team to deploy and manage a professional cloud environment, learning real DevOps and AWS best practices.\n2.3 Project Plan The project follows Agile Scrum over 3 months, divided into 4 sprints.\nSprint Structure\nSprint Planning\nSetup AWS foundational services (S3, Route 53, IAM)\nConfigure security (WAF, CloudFront)\nIntegrate backend (Lambda, API Gateway, RDS)\nTesting, optimization, and demo preparation\nDaily Stand-up 30-minute updates to address blockers and track status.\nSprint Review Review deliverables, demo on real AWS environment, fix issues.\nRetrospective Improve DevOps workflows and automation pipeline.\nTeam Roles\nProduct Owner: Business alignment, backlog prioritization\nScrum Master: Coordination, Agile process enforcement\nDevOps/Technical Team: Backend, infrastructure, CI/CD\nMentor / AWS Partner: Architecture validation, AI testing, cost \u0026amp; security review\nCommunication Rhythm\nDaily Stand-ups (23:00)\nWeekly Sync\nEnd-of-Sprint Demo\nKnowledge Transfer After the final sprint, the technical team will deliver hands-on training on operations, monitoring (Budgets, CloudWatch), scaling, and recovery procedures.\n2.4 Security Considerations Access Management MFA for admin users; IAM roles with least privilege; auditing through CloudTrail.\nInfrastructure Security\ndedicated VPC, services are restricted using resource policies; all public endpoints use HTTPS.\nData Protection\nS3 and RDS encryption; TLS data transfer; manual periodic backups.\nDetection \u0026amp; Monitoring\nCloudTrail, Config, and CloudWatch for visibility; GuardDuty for threat detection.\nIncident Response\nClear incident workflows with log collection, analysis, and periodic simulations.\n3. PROJECT ACTIVITIES \u0026amp; DELIVERABLES 3.1 Activities \u0026amp; Deliverables Table Phase Timeline Activities Deliverables Effort(day) Infrastructure Setup Week 1 – 2 Requirements gathering, architecture design, AWS configuration (S3, CloudFront, API, Lambda, RDS, Cognito), GitLab CI/CD setup Completed AWS Architecture, Ready Infrastructure, Active CI/CD 10 Frontend Development Week 3–5 UI/UX design, FE pages (Home, Catalog, Product Detail, Cart, Checkout), API integration Completed FE (Dev), Frontend connected to API 15 Backend \u0026amp; Database Week 6–9 Lambda APIs, RDS setup, order/user/product logic, Cognito IAM setup Stable API, validated data flow, full Frontend–Backend integration 20 Testing \u0026amp; Validation Week 10–11 Functional, security, performance testing, integration testing Test Report, Validated System 5 Production Launch Week 12 Deploy to production, domain \u0026amp; SSL setup, training \u0026amp; handover Live FFF Website, Documentation Package 5 3.2 Out of Scope The following items were discussed during the requirements definition phase, but were determined to be out of scope for the FFF Web Clothing project at the current stage.\nItems out of scope include:\nMobile App development for the system (Android/iOS). Integration of real-world inventory, shipping and logistics management systems (Fast Delivery, GHN, Viettel Post, etc.). Advanced administrative functions such as multi-level authorization, automatic revenue reporting, advanced statistical charts. Integration of third-party CRM (Customer Relationship Management) or ERP (Enterprise Resource Planning). Use of AWS services with higher, more expensive automatic security features. Integration of real-world payment gateways (VNPay, Momo, ZaloPay, Stripe, PayPal, etc.) Multilingual and multi-currency 3.3\tPATH TO PRODUCTION Phase 1 – Prototype (POC)\nActivities: Build a test version of FFF Web Sales with basic interface (Home, Category, Product Details, Cart).\nConnect backend via API Gateway – Lambda – DynamoDB.\nDeploy static website on Amazon S3 + CloudFront.\nConfigure admin account and demo trial order process.\nPhase 2 – Complete system and test (UAT)\nActivities:\nAdd user functions: login/register, authentication via AWS Cognito.\nAdd trial payment feature via sandbox.\nAdd monitoring with Amazon CloudWatch and error handling log.\nPerform internal user testing (User Acceptance Test).\nPhase 3 - Official Operation Deployment (Production)\nActivities:\nMove the entire system from the test environment to Production AWS. Configure Route53 for the official domain and SSL certificate via AWS Certificate Manager. Set up external security with AWS WAF. Optimize S3 capacity and CDN structure on CloudFront. Phase 4 – Stabilization \u0026amp; optimization after deployment\nActivities:\nMonitor actual AWS costs, optimize storage and logs.\nAdjust Lambda configuration to reduce cold start time.\nPerform periodic backups and test data recovery.\nUpdate operational documentation for the administration team.\nSummary\nThe FFF Web Sales system has been successfully deployed on the AWS Serverless platform with a cost-optimized, highly secure and scalable architecture. The stages were completed on schedule, ensuring that all functions were tested, refined and operated stably. The project is now ready to expand real users and integrate more advanced e-commerce features.\n4. AWS COST ESTIMATION Estimated monthly cost:\nRoute 53 : $1.00 AWS WAF : $5.00 CloudFront: $3.90 Amplify: $10.00 S3 (StaticData) : $0.50 S3 (Uploads): $0.75 S3 (Bucket): $0.75 AWS Lambda: $0.25 API Gateway: $3.50 Amazon Bedrock: $3.00 RDS: $21.00 IAM: Free CloudWatch: $2.00 SNS: $0.10 SES: $0.20 CloudFormation: Free GitLab CI/CD : $3.00 WS Config / Setup \u0026amp; Test migration tools $5.00 (1 lần) Estimated monthly total cost: ~ $50.00 – $55.00 USD KEY ASSUMPTIONS\nRegion: ap-southeast-1 (Singapore). User access: 100–200/month. The system is always running 24/7 but low load. Mostly API via Lambda. Small data (\u0026lt;30GB total). CI/CD 1–2 deployments per week. Free-tier is valid for the first 12 months. AI is used for demo purposes, not large-scale inference.\nSUGGESTED COST OPTIMISATION\nEnable S3 Intelligent-Tiering to automatically move less frequently accessed data. Limit CloudWatch Logs to 14–30 days. Use AWS Budgets to alert if it exceeds $40/month. If deploying long-term → consider Savings Plan for Lambda (30–40% reduction).\n5. Team Partner Executive Sponsor Name: Nguyen Gia Hung Title: FCJ Vietnam Training Program Director Description: Responsible for overall oversight of the FCJ internship program\nEmail/contact information: hunggia@amazon.com|\nProject Stakeholders Name: Van Hoang Kha Title: Support Teams Description: Responsible for overall supervision of the FCJ internship program as the Executive Support person.\nEmail/Contact information: Khab9thd@gmail.com\nPartner Project Team (Furious Five Internship Team)\nName: Duong Minh Duc Title: Project Team Leader\nDescription: Manage progress, coordinate work between the team and mentor, Manage AWS infrastructure deployment (S3, Lambda, IAM)\nEmail/Contact information: ducdmse182938@fpt.edu.vn\nName: Quach Nguyen Chi Hung\nTitle: Member\nDescription: In charge of UI/UX and user interface\nEmail/Contact information: bacon3632@gmail.com\nName: Nguyen Tan Xuan\nTitle: Member\nDescription: Responsible for Backend and server logic processing\nEmail/Contact information: xuanntse184074@fpt.edu.vn\nName: Nguyen Hai Dang\nTitle: Member\nDescription: Manage AWS infrastructure deployment (S3, Lambda, IAM) and AI chat bot integration\nEmail/Contact information: dangnhse184292@fpt.edu.vn\nName: Pham Le Huy Hoang\nTitle: Member\nDescription: Testing, quality assurance and GitLab CI/CD integration, and AI chat bot integration\nEmail/Contact information: hoangplhse182670@fpt.edu.vn\nProject Escalation Contacts Name: Duong Minh Duc\nTitle: Project Team Leader\nDescription: Represent the internship team to contact the mentor and sponsor directly\nEmail/Contact information: ducdmse182938@fpt.edu.vn\n6. RESOURCES \u0026amp; COST ESTIMATES Resources Role Responsibilities Fee (USD)/Hour Solution Architect(1) Design overall solution, ensure technical feasibility, select appropriate AWS service 35 Cloud Engineer(2) Deploy AWS infrastructure, configure services (S3, IAM\u0026hellip;), test and optimize system 20 Project Manager (1) Monitor progress, coordinate team, manage scope and risk for the project. 15 Support / Documentation (1) Prepare communication documents, user manuals and summary reports. 10 Estimate costs by project phase Project Phase Solution Architect (hrs) 2 Engineers (hrs) Project Manager (hrs) Project Management/Suppor(hrs) Total Hours Survey \u0026amp; Solution Design 53 40 13 13 119 Implementation \u0026amp; Testing 67 160 21 19 267 Handover \u0026amp; Support 27 53 21 19 120 Total Hours 147 253 55 51 506 Total Amount $5145 $5060 $825 $510 $11540 Cost Contribution Allocation Party Contribution (USD) % Contribution Customer 4616 40% Partner (Furious Five) 2308 20% AWS 4616 40% 7.\tACCEPTANCE Since this project is currently at the presentation stage and has not yet been formally evaluated by a customer, the following acceptance process is proposed for future delivery phases:\n7.1 Acceptance Criteria (Proposed) A deliverable will be considered acceptable when it meets the following criteria:\nFunctional features work as specified (authentication, recipe management, social features, AI functions). All APIs respond correctly and integrate with AWS services (Lambda, API Gateway, RDS, S3). Security requirements are met (JWT verification, HTTPS, access control, data encryption). UI works as expected on supported devices. No critical errors appear during test execution. 7.2 ACCEPTANCE PROCESS Review period: 8 business days for evaluation and testing. If accepted → Deliverable is signed off. If issues are found → A rejection notice will be issued with feedback. Fixes will be applied and a revised version will be resubmitted for review. If no response is received by the end of the review period → Deliverable is deemed accepted. After completing each milestone, the team submits the deliverables and documentation. Tải file .docx "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Implement user-side CRUD operations: edit user profile, add/delete orders. Hold a group meeting at Phuc Long to discuss project plan and connecting AWS Bedrock Chatbot to the backend. Test and deploy changes to staging for verification. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Add edit profile feature for users (avatar, address, contact info) - Test frontend and API for profile update - Write unit tests for update endpoint 10/11/2025 10/11/2025 3 - Implement user-side create/delete order functionality - Add business validation and handle edge cases - Write integration tests for order APIs 10/12/2025 10/12/2025 4 - Group meeting at Phuc Long: discuss Bedrock Chatbot integration with backend - Draft API design, auth flow, and security considerations - Record action items and timeline 10/13/2025 10/13/2025 5 - Prototype chatbot integration with backend (mock/stub) - Perform end-to-end chat flow tests - Prepare integration guide and sample code for the team 10/14/2025 10/14/2025 Week 10 Achievements: User-side CRUD features implemented\nAdded profile edit (avatar, address, contact details) with frontend and API support Implemented create/delete order flows with validation and error handling Added unit and integration tests to cover user endpoints Team meeting at Phuc Long and Bedrock integration plan\nDiscussed Chatbot integration architecture and identified integration points and authentication flow Produced an action list and timeline for implementing Bedrock-based Chatbot Basic prototype and documentation\nBuilt a mock Chatbot-backend prototype and verified end-to-end chat flows Prepared integration guide and example snippets for the team Improved user experience and integration readiness\nEnabled users to manage orders directly from the UI Laid groundwork for Chatbot-assisted shopping features "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Add credit card transaction capability to the project to enable user payments securely. Attend AWS Cloud Mastery Series (Bitexco Financial Tower) and apply DevOps/CI-CD and observability learnings to the project. Complete integration, testing, and deployment of payment flows and document the implementation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attend AWS Cloud Mastery Series at Bitexco Financial Tower (full day) - Sessions covered: DevOps mindset, CI/CD (CodeCommit/CodeBuild/CodeDeploy/CodePipeline), IaC (CloudFormation/CDK), containers (ECR/ECS/EKS), observability (CloudWatch/X-Ray), and best practices. 11/17/2025 11/17/2025 3 - Design and integrate credit card payment flow (choose gateway / tokenization) - Implement secure server-side payment endpoint and token handling - Ensure sensitive data is not stored in cleartext 11/18/2025 11/18/2025 4 - Implement client-side payment UI and connect to backend tokenization API - Add server-side validation, logging and retry/error handling - Add monitoring for payment failures and success rates 11/19/2025 11/19/2025 5 - End-to-end testing of payment flows (success, decline, network failures) - Run security checks and ensure compliance considerations (tokenization, TLS) - Fix reported issues from tests 11/20/2025 11/20/2025 6 - Deploy payment features to staging - Demo to team and collect feedback - Document implementation, runbooks and monitoring alerts for production rollout 11/21/2025 11/21/2025 Week 11 Achievements: Attended AWS Cloud Mastery Series (Bitexco Financial Tower) on 11/17/2025\nParticipated in sessions covering DevOps mindset, CI/CD pipelines (CodeCommit, CodeBuild, CodeDeploy, CodePipeline), IaC (CloudFormation, CDK), container services (ECR/ECS/EKS), and observability (CloudWatch, X-Ray). Gained actionable ideas for improving our CI/CD workflows and monitoring strategy. Implemented secure credit card payment capability\nDesigned and integrated payment flow using tokenization (no raw card storage) Implemented backend payment endpoints, client integration, and server-side validation Added logging, retries, and monitoring for payment success/failure rates Verified payment flows with end-to-end tests and security checks\nTested success, decline, and network-failure scenarios Applied TLS, tokenization and basic compliance best practices Deployed payment features to staging and documented the integration\nPrepared runbooks, monitoring alerts, and integration guide for the team Demoed features to the team and collected feedback for production rollout "
},
{
	"uri": "http://localhost:1313/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Review the project end-to-end and fix any bugs prior to mentor presentation. Deploy backend, database, and frontend into AWS (staging and production as appropriate) with repeatable CI/CD pipelines. Prepare demo, runbooks, and monitoring so mentors can review a stable, observable system. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Code review and bug triage: run static analysis, prioritize critical/major bugs, create fix plan 11/24/2025 11/24/2025 2 - Fix high-priority backend issues: unit \u0026amp; integration tests, database query optimizations, handle error paths and edge cases 11/25/2025 11/25/2025 3 - Prepare deployment artifacts: containerize services, build artifacts, verify infra-as-code templates (CloudFormation/CDK), and configure CI pipelines for staging deploys 11/26/2025 11/26/2025 4 - Deploy backend \u0026amp; database to AWS staging (or production if ready): configure Secrets Manager/SSM, load balancer / ALB, autoscaling, and backups; validate connections and migrations 11/27/2025 11/27/2025 5 - Deploy frontend (S3 + CloudFront or hosting), run end-to-end smoke tests, run performance and load checks, prepare demo script and present rehearsal to team 11/28/2025 11/28/2025 Week 12 Achievements: Completed code review and resolved critical/major bugs blocking presentation.\nBackend and database deployed to AWS staging (and production where approved)\nCI/CD pipelines validated for automated builds and deploys to staging Database migrations applied and backups configured Frontend deployed and configured behind CDN (S3 + CloudFront) with routing and TLS\nMonitoring and observability configured\nApplication logs centralized, metrics and basic dashboards created, and alerts configured for critical failures Performed end-to-end smoke tests and a rehearsal demo for mentors\nPrepared demo script, runbook, and rollback steps Project is presentation-ready with documented deployment steps and testing results.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.03-cdk-bootstrap/",
	"title": "CDK Bootstrap",
	"tags": [],
	"description": "",
	"content": "Overview AWS CDK Bootstrap prepares your AWS account for deploying CDK applications by creating essential infrastructure resources. This is a one-time setup process per AWS account/region combination that establishes the foundation for all CDK deployments.\nWhat CDK Bootstrap Creates The bootstrap process provisions the following AWS resources in your account:\n1. S3 Bucket (CDK Assets Bucket)\nStores Lambda function deployment packages Stores CloudFormation templates Stores file assets referenced by your CDK stacks Naming pattern: cdk-hnb659fds-assets-{ACCOUNT-ID}-{REGION} Versioning enabled for rollback support 2. CloudFormation Stack (CDKToolkit)\nMain infrastructure stack containing all bootstrap resources Stack name: CDKToolkit Manages lifecycle of bootstrap resources 3. IAM Roles\ncdk-hnb659fds-cfn-exec-role: CloudFormation execution role for deploying stacks cdk-hnb659fds-deploy-role: Role used by CDK CLI during deployment cdk-hnb659fds-file-publishing-role: Role for uploading assets to S3 cdk-hnb659fds-lookup-role: Role for reading environment context 4. SSM Parameters\n/cdk-bootstrap/hnb659fds/version: Stores bootstrap version number Used for compatibility checking Infrastructure Stacks Overview After bootstrapping, our OJT E-commerce project will deploy multiple CDK stacks defined in OJT_infrastructure/lib/stacks:\nStack File Description NetworkStack network-stack.ts VPC, Subnets, NAT Gateway, Security Groups StorageStack storage-stack.ts S3 Buckets (Images, Logs) AuthStack auth-stack.ts Cognito User Pool \u0026amp; Identity Pool DatabaseStack database-stack.ts RDS SQL Server + Secrets Manager ApiStack api-stack.ts API Gateway + 11 Lambda Modules FrontendStack frontend-stack.ts S3 + CloudFront (OAC) MonitoringStack monitoring-stack.ts CloudWatch Dashboard \u0026amp; Alarms These stacks depend on the bootstrap infrastructure to deploy successfully.\nLab Instructions: Bootstrap Your AWS Account Prerequisites Before beginning this lab, ensure you have:\nAWS CLI installed and configured (see section 5.02) AWS CDK CLI installed (see section 5.02) Valid AWS credentials configured IAM user with AdministratorAccess permissions or equivalent Terminal/PowerShell access to the project directory Step 1: Navigate to Infrastructure Directory cd OJT_infrastructure Step 2: Get Your AWS Account ID aws sts get-caller-identity --query Account --output text ```bash # Bootstrap CDK for ap-southeast-1 region cdk bootstrap aws://YOUR_ACCOUNT_ID/ap-southeast-1 # Example: cdk bootstrap aws://123456789012/ap-southeast-1 Expected Output:\nBootstrapping environment aws://123456789012/ap-southeast-1...\rEnvironment aws://123456789012/ap-southeast-1 bootstrapped. ![CDK Bootstrap Success] Screenshot: Terminal showing CDK bootstrap completed successfully\nStep 4: Verify Bootstrap via CLI # Check bootstrap version aws ssm get-parameter \\ --name /cdk-bootstrap/hnb659fds/version \\ --region ap-southeast-1 \\ --query Parameter.Value \\ --output text # Expected output: A version number (e.g., 19) ![Bootstrap Verification CLI] Screenshot: Terminal showing bootstrap version verification\nStep 5: Verify on AWS Console Confirm resources in the AWS Management Console.\n5.1. Verify CloudFormation Stack\nOpen [CloudFormation Console] Look for stack named CDKToolkit Status should be CREATE_COMPLETE Click on Resources tab to view created resources ![CloudFormation CDKToolkit Stack] Screenshot: CloudFormation console showing CDKToolkit stack\n5.2. Verify S3 Bucket\nOpen S3 Console Find bucket: cdk-hnb659fds-assets-{ACCOUNT-ID}-ap-southeast-1 Verify bucket properties: Versioning: Enabled ![S3 Bucket Console] Screenshot: S3 console showing CDK assets bucket\n5.3. Verify IAM Roles\nOpen IAM Roles Console Search for: cdk-hnb659fds Verify the following roles exist: cdk-hnb659fds-cfn-exec-role-{ACCOUNT-ID}-ap-southeast-1 cdk-hnb659fds-deploy-role-{ACCOUNT-ID}-ap-southeast-1 cdk-hnb659fds-file-publishing-role-{ACCOUNT-ID}-ap-southeast-1 cdk-hnb659fds-lookup-role-{ACCOUNT-ID}-ap-southeast-1 ![IAM Roles Console] Screenshot: IAM console showing CDK bootstrap roles\nUnderstanding Bootstrap Resources S3 Bucket Details The CDK assets bucket serves as a central repository for all deployment artifacts:\nPurpose: Stores CloudFormation templates, Lambda deployment packages, and static assets Lifecycle: Persists across deployments, enabling rollback capabilities Security: Encrypted at rest, bucket policies restrict access to authorized roles Naming: Deterministic naming based on account ID and region IAM Roles Details 1. CloudFormation Execution Role (cfn-exec-role)\nUsed by CloudFormation to create/update/delete stack resources Has broad permissions to manage AWS resources Trust relationship with CloudFormation service 2. Deploy Role (deploy-role)\nUsed by CDK CLI during cdk deploy operations Can assume the CloudFormation execution role Has permissions to upload assets and initiate deployments 3. File Publishing Role (file-publishing-role)\nUploads Lambda packages and assets to S3 Has S3 write permissions to the assets bucket 4. Lookup Role (lookup-role)\nReads environment context (VPCs, subnets, etc.) Read-only permissions for resource lookups Used during synthesis for context queries Cost Analysis Bootstrap Resources Cost One-time Setup:\nCloudFormation stack creation: Free IAM roles creation: Free SSM parameters: Free Ongoing Monthly Costs:\nResource Usage Cost (Estimated) S3 Storage \u0026lt; 1 GB (typical) $0.023/GB = ~$0.02/month S3 Requests Minimal (GET/PUT) ~$0.01/month IAM Roles No charge $0.00 Total Estimated Cost: ~$0.03/month (negligible)\nNote: For serverless applications, costs remain minimal as we only store Lambda deployment packages in S3.\nTroubleshooting Issue: Bootstrap fails with permission error\n# Ensure your IAM user has AdministratorAccess aws iam list-attached-user-policies --user-name YOUR_USERNAME Issue: Region mismatch\n# Verify your default region aws configure get region # Bootstrap specific region cdk bootstrap aws://ACCOUNT_ID/ap-southeast-1 Issue: CDK version mismatch\n# Update CDK CLI npm update -g aws-cdk # Verify version cdk --version Lab Completion Checklist Ensure you have completed all steps:\nStep 1: Navigate to OJT_infrastructure directory Step 2: Retrieved your AWS account ID Step 3: Executed CDK bootstrap command Step 4: Verified bootstrap version via CLI Step 5.1: Verified CDKToolkit CloudFormation stack Step 5.2: Verified CDK assets S3 bucket Step 5.3: Verified IAM roles created Summary In this lab, you successfully:\nExecuted CDK bootstrap command for your AWS account (ap-southeast-1) Created CDKToolkit CloudFormation stack Provisioned CDK assets S3 bucket for storing Lambda packages Created necessary IAM roles for CDK deployments Verified all resources via CLI and AWS Console Your AWS account is now ready to deploy serverless CDK applications, including the infrastructure stacks for the OJT E-commerce project.\nNext Steps Proceed to [Deploy Core Infrastructure] to deploy VPC, Database, Storage, and Auth stacks.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nEvent Report: \u0026ldquo;AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\u0026rdquo; Time and Topic Time: Saturday, November 15, 2025 (8:30 AM – 12:00 PM) Topic: AI/ML/GenAI on AWS Event Objectives Provide an overview of AWS AI/ML services. Introduce Amazon SageMaker in detail – the end-to-end ML platform. Deep dive into Generative AI (GenAI) and Foundation Models through Amazon Bedrock. Guide through Prompt Engineering techniques and Retrieval-Augmented Generation (RAG) architecture. Speaker List Detailed speaker information was not available in the original data; assumed to be Solution Architects from AWS Vietnam. Key Highlights 1. AWS AI/ML Services Overview Amazon SageMaker: Complete Machine Learning lifecycle including data preparation, training, tuning, model deployment, and integrated MLOps. Live Demo: Hands-on operations with SageMaker Studio. 2. Generative AI with Amazon Bedrock Foundation Models: Comparison and guidance on selecting between models like Claude, Llama, Titan. Prompt Engineering: Advanced techniques such as Chain-of-Thought reasoning and Few-shot learning. Retrieval-Augmented Generation (RAG): Architecture and integration with Knowledge Base to enhance accuracy and context for GenAI. Bedrock Agents \u0026amp; Guardrails: Building multi-step workflows and Guardrails mechanisms (content moderation) to ensure safety. Key Takeaways GenAI and ML Skills End-to-end ML Deployment: Mastered the fundamental steps to build and deploy ML models on SageMaker effectively. Bedrock Mastery: Gained deep understanding of Bedrock components and capabilities of different Foundation Models. RAG Techniques: Grasped that RAG architecture is the key element for bringing GenAI into real-world enterprise applications with private data. Application to Work Building GenAI Proof-of-Concept (PoC): Using Amazon Bedrock to quickly experiment with GenAI use cases (e.g., document summarization, chatbots based on internal knowledge) with RAG techniques. Prompt Optimization: Applying learned Prompt Engineering techniques to improve response quality from large language models. Event Experience In-depth Practice: The Live Demos on SageMaker and Bedrock were highly valuable, providing real-world insights into service deployment. Focus on Details: The event delved deep into technical aspects like model selection and prompt optimization, giving me more solid knowledge compared to events that are merely introductory. This event provided a solid foundation for me to start working with Machine Learning and Generative AI projects on AWS.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - OpenAI open weight models now available on AWS On August 5, 2025, AWS announced that two of OpenAI’s latest open-weight models — gpt-oss-120b and gpt-oss-20b — are now accessible through Amazon Bedrock and Amazon SageMaker JumpStart. This release marks a significant milestone, as it’s the first time OpenAI has provided public access to model weights since GPT-2, opening the door to greater flexibility and customization.\nBlog 2 - OpenSecrets uses AWS to transform political transparency through enhanced data matching OpenSecrets is a nonpartisan, independent nonprofit organization whose mission is to serve as the trusted authority on money in American politics. It pursues this mission by providing comprehensive and reliable data, analysis, and tools for policymakers, storytellers, and citizens. Its vision is for Americans to use data on money in politics to create a more vibrant, representative, and responsive democracy.\nBlog 3 - An Analysis of AWS\u0026rsquo;s Leadership in the 2025 Gartner Magic Quadrant Reports As I analyze the recent AWS blog posts detailing their recognition across multiple Gartner Magic Quadrant reports, I find myself reflecting on what truly constitutes cloud leadership in today\u0026rsquo;s rapidly evolving technological landscape. Having closely followed AWS\u0026rsquo;s journey over the past decade, I believe their consistent performance across these evaluations reveals more than just market dominance—it demonstrates a profound understanding of enterprise needs across various domains. In my assessment, achieving leadership status in even one Magic Quadrant is noteworthy, but maintaining this position across five different categories while earning the highest placement in \u0026ldquo;Ability to Execute\u0026rdquo; for Strategic Cloud Platform Services for 15 consecutive years is, in my opinion, unprecedented in the cloud industry. This consistent excellence suggests a deep-rooted culture of customer obsession and innovation that permeates their entire organization.\nBlog 4 - From Flexibility to Framework: Enforcing Tool Order in MCP Servers AWS Amplify has rolled out important updates to its JavaScript library, making it more lightweight and efficient. Key categories such as Auth, Storage, Notifications, and Analytics have seen major reductions in bundle size, which directly translates to faster load times and better performance for developers and their users.\nThese improvements aren’t just technical tweaks—they are the result of listening to the Amplify developer community. By focusing on bundle size optimization and better tree-shaking support, Amplify is making sure that apps built with the library meet modern performance expectations.\nBlog 5 - Multi Agent Collaboration with Strands As autonomous systems evolve, collaboration between multiple agents is moving from theoretical to essential. With agents gaining advanced reasoning, adaptability, and tool use, the question is no longer “Can one agent solve a task?” but “How can many agents work together effectively?”\nBlog 6 - From Flexibility to Framework: Enforcing Tool Order in MCP Servers The Model Context Protocol (MCP) was created to bring consistency to how applications interact with Generative AI models. Instead of piecing together separate integrations for every model or hosting environment, MCP provides a standardized communication layer.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.04-configure-stacks/",
	"title": "Configure Infrastructure Stacks",
	"tags": [],
	"description": "",
	"content": "Configure Infrastructure Stacks OJT E-commerce Project Overview Introduction OJT E-commerce is a serverless e-commerce platform built entirely on AWS Cloud. The project uses a serverless architecture with Lambda functions replacing traditional Spring Boot backend, leveraging AWS managed services to ensure scalability, security, and cost optimization.\nSystem Architecture The project is designed with a 7-Stack Architecture featuring clear layers:\n┌─────────────────────────────────────────────────────────────┐\r│ OJT E-commerce Platform │\r├─────────────────────────────────────────────────────────────┤\r│ Layer 1: Network (Foundation) │\r│ └─ Network Stack: VPC, Subnets, NAT Gateway, Security Groups│\r├─────────────────────────────────────────────────────────────┤\r│ Layer 2: Data \u0026amp; Storage │\r│ ├─ Storage Stack: S3 Buckets (Images, Logs) │\r│ └─ Database Stack: RDS SQL Server, Secrets Manager │\r├─────────────────────────────────────────────────────────────┤\r│ Layer 3: Authentication │\r│ └─ Auth Stack: Cognito User Pool, Identity Pool │\r├─────────────────────────────────────────────────────────────┤\r│ Layer 4: Application \u0026amp; Business Logic │\r│ └─ API Stack: API Gateway, 11 Lambda Modules │\r├─────────────────────────────────────────────────────────────┤\r│ Layer 5: Content Delivery │\r│ └─ Frontend Stack: S3 Static Hosting, CloudFront CDN │\r├─────────────────────────────────────────────────────────────┤\r│ Layer 6: Monitoring \u0026amp; Observability │\r│ └─ Monitoring Stack: CloudWatch Dashboard, Alarms │\r└─────────────────────────────────────────────────────────────┘ Technologies Used Infrastructure as Code AWS CDK (TypeScript): Infrastructure management with code CloudFormation: Underlying template engine for CDK Git: Version control for infrastructure code Backend Services API Gateway REST API: RESTful API endpoint Lambda Functions: Serverless compute for business logic RDS SQL Server: Relational database S3: Object storage for product images and frontend Secrets Manager: Secure credential storage Security \u0026amp; Authentication Cognito User Pool: User authentication \u0026amp; management JWT Authentication: Custom JWT-based auth in Lambda VPC: Network isolation with public/private/isolated subnets Security Groups: Firewall rules for resources IAM Roles \u0026amp; Policies: Fine-grained access control Content Delivery \u0026amp; Networking CloudFront: CDN with Origin Access Control (OAC) NAT Gateway: Internet access for private subnets VPC Endpoints: Private connectivity to AWS services Monitoring \u0026amp; Operations CloudWatch Logs: Centralized logging for Lambda functions CloudWatch Metrics: Performance metrics tracking CloudWatch Dashboard: Visualization of metrics CloudWatch Alarms: Real-time monitoring alerts Project Directory Structure OJT/\r├── OJT_infrastructure/ # AWS CDK Infrastructure\r│ ├── bin/\r│ │ └── infrastructure.ts # CDK app entry point\r│ ├── lib/\r│ │ └── stacks/ # Stack definitions\r│ │ ├── network-stack.ts # VPC, Subnets, NAT Gateway\r│ │ ├── storage-stack.ts # S3 Buckets\r│ │ ├── auth-stack.ts # Cognito User Pool\r│ │ ├── database-stack.ts # RDS SQL Server\r│ │ ├── api-stack.ts # API Gateway + Lambda\r│ │ ├── frontend-stack.ts # S3 + CloudFront\r│ │ └── monitoring-stack.ts # CloudWatch\r│ ├── .env.example # Environment template\r│ ├── package.json # Node.js dependencies\r│ └── tsconfig.json # TypeScript configuration\r│\r├── OJT_lambda/ # Lambda Functions (63 APIs)\r│ ├── auth/ # Authentication (4 functions)\r│ ├── products/ # Products (12 functions)\r│ ├── product-details/ # Product Details (7 functions)\r│ ├── cart/ # Cart (6 functions)\r│ ├── orders/ # Orders (9 functions)\r│ ├── categories/ # Categories (6 functions)\r│ ├── brands/ # Brands (5 functions)\r│ ├── banners/ # Banners (7 functions)\r│ ├── ratings/ # Ratings (3 functions)\r│ ├── users/ # Users (3 functions)\r│ ├── images/ # Images (1 function)\r│ └── shared/ # Shared utilities\r│\r├── OJT_frontendDev/ # Frontend (React + Vite)\r│ ├── src/ # Source code\r│ └── public/ # Static assets\r│\r└── database/ # Database Scripts\r├── schema/ # SQL schema files\r├── migrations/ # Migration scripts\r└── seeds/ # Sample data Stack Architecture Overview 1. Network Stack (Deploy Order: 1) Purpose: Create VPC and network infrastructure\nMain resources:\nVPC: 10.0.0.0/16 CIDR block Public Subnets: 2 AZs - Internet-facing resources Private Subnets: 2 AZs - Lambda functions, internal services Isolated Subnets: 2 AZs - RDS database (no internet) NAT Gateway: 1 instance (cost optimized) Internet Gateway: VPC internet access Security Groups: Lambda SG, RDS SG VPC Endpoints: S3, Secrets Manager Estimated cost: ~$23/month (NAT Gateway)\n2. Storage Stack (Deploy Order: 2) Purpose: Create S3 buckets for images and logs\nMain resources:\nImages Bucket: Product images storage Versioning enabled Lifecycle rules for cost optimization Logs Bucket: CloudFront access logs Auto-delete after 90 days Estimated cost: ~$1-3/month\n3. Auth Stack (Deploy Order: 2) Purpose: User authentication with Cognito (Optional)\nMain resources:\nCognito User Pool: User registration and authentication Email verification required Password policy: 8+ chars, mixed case, numbers, symbols Cognito User Pool Client: Frontend authentication Cognito Identity Pool: AWS credentials for authenticated users Estimated cost: $0/month (Free tier: 50,000 MAU)\nNote: This stack is optional. The project also supports custom JWT authentication in Lambda.\n4. Database Stack (Deploy Order: 2) Purpose: SQL Server database with Secrets Manager\nMain resources:\nRDS SQL Server Express 2019: Instance: db.t3.micro (cost optimized) Storage: 20 GB gp3 SSD Multi-AZ: Disabled (dev/staging) Backup: 1 day retention Secrets Manager: Database credentials Auto-generated strong password Optional rotation Estimated cost: ~$15/month (optimized from $54)\n5. API Stack (Deploy Order: 3) Purpose: REST API with API Gateway and Lambda functions\nMain resources:\nAPI Gateway REST API: 63 endpoints across 11 modules CORS enabled CloudWatch logging Lambda Functions (11 modules): Auth, Products, ProductDetails, Cart, Orders Categories, Brands, Banners, Ratings, Users, Images Runtime: Node.js 20.x Memory: 128-512 MB (optimized) VPC: Private subnets Estimated cost: ~$2-5/month\n6. Frontend Stack (Deploy Order: 4) Purpose: Static website hosting with CDN\nMain resources:\nS3 Bucket: React build files CloudFront Distribution: Origin Access Control (OAC) HTTPS only Gzip compression Estimated cost: ~$1-2/month\n7. Monitoring Stack (Deploy Order: 5) Purpose: Monitoring, logging, and alerting\nMain resources:\nCloudWatch Dashboard: System metrics visualization CloudWatch Alarms: API Gateway 5xx errors Lambda errors RDS CPU utilization CloudWatch Log Groups: Lambda function logs Estimated cost: ~$1-2/month\nStack Dependencies Flow Network Stack (VPC, Subnets, NAT Gateway)\r↓\r┌─────────────────────────────────────┐\r│ Storage Stack Auth Stack │\r│ (S3 Buckets) (Cognito) │\r│ │\r│ Database Stack │\r│ (RDS SQL Server) │\r└─────────────────────────────────────┘\r↓\rAPI Stack (API Gateway + Lambda)\r↓\rFrontend Stack (S3 + CloudFront)\r↓\rMonitoring Stack (CloudWatch) Total Estimated Cost Development Environment:\nService Cost/month Notes NAT Gateway $23 1 instance RDS SQL Server $15 t3.micro Lambda $2 11 modules, 128MB S3 Storage $1.25 Images + Frontend CloudFront $1.50 CDN distribution CloudWatch $1.50 Dashboard + Logs Cognito $0 Free tier (50K MAU) API Gateway $0-3 Free tier (1M requests) TOTAL ~$44/month 60% reduction from $111 Configuration Guide Step 1: Navigate to Infrastructure Directory cd OJT_infrastructure Step 2: Install Dependencies npm install Step 3: Configure Environment Variables 1. Copy environment template\ncopy .env.example .env 2. Edit .env file with your values\n# AWS Configuration AWS_ACCOUNT_ID=123456789012 AWS_REGION=ap-southeast-1 # Database Configuration DB_NAME=demoaws DB_USERNAME=admin DB_PASSWORD=YourSecurePassword123! # Application Configuration APP_NAME=OJT-Ecommerce ENVIRONMENT=dev # JWT Secret JWT_SECRET=your-super-secret-jwt-key-change-this-in-production 3. Verify AWS Account ID\naws sts get-caller-identity Output:\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/your-username\u0026#34; } Step 4: Validate Configuration 1. Compile TypeScript\nnpm run build 2. List all CDK stacks\nnpx cdk list Expected output:\nOJT-NetworkStack\rOJT-StorageStack\rOJT-AuthStack\rOJT-DatabaseStack\rOJT-ApiStack\rOJT-FrontendStack\rOJT-MonitoringStack 3. Synthesize CloudFormation templates\nnpx cdk synth The cdk.out/ folder is created with CloudFormation templates.\nStep 5: Review Stack Code (Optional) Network Stack (lib/stacks/network-stack.ts):\nVPC with 10.0.0.0/16 CIDR Public, Private, Isolated subnets 1 NAT Gateway (cost optimized) Security Groups for Lambda and RDS Database Stack (lib/stacks/database-stack.ts):\nRDS SQL Server Express 2019 db.t3.micro instance (cost optimized) Secrets Manager for credentials 1-day backup retention API Stack (lib/stacks/api-stack.ts):\nAPI Gateway REST API 11 Lambda modules with placeholder code VPC integration for database access Configuration Checklist Before deploying, verify the following:\nEnvironment Configuration\nAWS Account ID verified and updated in .env Region set to ap-southeast-1 Database credentials configured JWT secret configured Dependencies\nNode.js 20.x installed AWS CLI configured with credentials AWS CDK CLI installed globally npm dependencies installed (npm install) Validation\nTypeScript compilation successful (npm run build) CDK list shows all 7 stacks CDK synth generates CloudFormation templates Preparation\nCDK bootstrap completed AWS account has AdministratorAccess permissions Next Steps After completing configuration and validation, continue to:\nDeploy all stacks to AWS In the next step, you will:\nDeploy Network Stack (VPC, Subnets, NAT Gateway) Deploy Storage Stack (S3 Buckets) Deploy Auth Stack (Cognito - optional) Deploy Database Stack (RDS SQL Server) Deploy API Stack (API Gateway + Lambda) Deploy Frontend Stack (S3 + CloudFront) Deploy Monitoring Stack (CloudWatch) "
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nEvent Report: \u0026ldquo;AWS Cloud Mastery Series #2: DevOps on AWS\u0026rdquo; Time and Topic Time: Monday, November 17, 2025 (1:00 PM – 4:30 PM) Topic: DevOps on AWS Event Objectives Provide deep insights into DevOps principles, processes, and best practices on the AWS platform. Introduce and guide the use of AWS Developer Tools suite (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) to build automated CI/CD pipelines. Focus on advanced deployment strategies like Blue/Green Deployment and Canary Release to minimize risks. Explore how to integrate Security (DevSecOps) and Monitoring into the DevOps cycle. Speaker List Detailed speaker information was not available in the original data; assumed to be Solution Architects from AWS Vietnam. Key Highlights 1. DevOps Principles and AWS Developer Tools DevOps Culture \u0026amp; Principles: Focus on automation, monitoring, and continuous improvement. Building CI/CD: Guide on using AWS CodePipeline to create a continuous deployment flow from code commit to production. Infrastructure as Code (IaC): Introduction to AWS CloudFormation and AWS CDK for managing infrastructure resources with code. 2. Advanced Deployment Strategies Zero-Downtime Deployment: Analysis of advanced deployment techniques: Blue/Green Deployment: Deploy new version alongside the old version, then switch traffic. Canary Release: Deploy new version to a small subset of users before wide rollout. AWS Services in Action: How to use AWS services like Amazon ECS, Amazon EKS, and AWS Lambda to support these strategies. 3. Monitoring, Logging, and DevSecOps Observability: Using Amazon CloudWatch and AWS X-Ray to collect metrics, logs, and traces for quick issue detection and resolution. DevSecOps: Integrating security tools (e.g., Amazon Inspector, Amazon GuardDuty) into Build and Deploy stages for early vulnerability detection. Key Takeaways Core DevOps Skills Professional CI/CD Design: Learned how to design a robust, resilient CI/CD Pipeline using AWS toolset. Deployment Risk Minimization: Understood the pros and cons of Blue/Green and Canary Release, and when to apply each strategy to ensure Zero Downtime. Security and Observability DevSecOps Mindset: Grasped the importance of \u0026ldquo;Shift Left\u0026rdquo; - integrating security as early as possible in the development cycle. Observability: Understood the difference between Monitoring and Observability, and how to use Logs, Metrics, Traces for comprehensive system performance insights. Application to Work Deployment Automation: Deploy CodePipelines for new projects to automate the entire Build, Test, and Deploy process. Improve Go-Live Strategy: Propose and apply Canary Release strategies for critical features to ensure stability and gather early user feedback. Enhance Observability: Integrate X-Ray for end-to-end request tracing, helping debug distributed applications (microservices) faster. Event Experience This event was a practical learning session, emphasizing the transition from theory to practice in the AWS environment.\nReal-world Value: Deep diving into advanced deployment strategies provided directly applicable knowledge for production environments. Tool Ecosystem: I gained better understanding of how tools in the AWS ecosystem (Code* services, CloudWatch, X-Ray) work together to create a complete DevOps workflow. This event strengthened my knowledge of building, deploying, and operating software efficiently, securely, and automatically on AWS.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18th, 2025\nLocation: 32th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescripition: Briefly explain Generative AI, Data Analytics, Migration \u0026amp; Modernization, as well as security to make the system scalable, secure and reliable.\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: A comprehensive workshop focusing on AWS Well-Architected Security Pillar, covering IAM best practices, detection and monitoring, infrastructure protection, data protection, and incident response strategies.\nEvent 3 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:30 AM, November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: Deep dive into AI/ML services on AWS, focusing on Amazon SageMaker for end-to-end ML lifecycle, Amazon Bedrock for Generative AI, and advanced techniques like RAG and Prompt Engineering.\nEvent 4 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate \u0026amp; Time: 1:00 PM, November 17, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: Comprehensive overview of DevOps practices on AWS, including CI/CD pipelines with AWS Developer Tools, advanced deployment strategies like Blue/Green and Canary releases, and DevSecOps integration.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.05-configure-api-lambda/",
	"title": "Configure API &amp; Lambda",
	"tags": [],
	"description": "",
	"content": "Overview Sau khi deploy infrastructure, bạn cần cấu hình API Gateway routes và Lambda functions để xử lý business logic. Dự án OJT E-commerce sử dụng kiến trúc 2-step deployment: Infrastructure (CDK) và Lambda Code (riêng biệt).\nAPI Architecture API Gateway (REST API)\r↓\r┌─────────────────────────────────────────────────────────────────┐\r│ 11 Lambda Modules (63 APIs) │\r├─────────┬─────────┬─────────┬─────────┬─────────┬───────────────┤\r│ Auth │Products │ Cart │ Orders │Categories│ Brands │\r│(4 APIs) │(12 APIs)│(6 APIs) │(9 APIs) │ (6 APIs) │ (5 APIs) │\r├─────────┼─────────┼─────────┼─────────┼─────────┼───────────────┤\r│ Banners │ Ratings │ Users │ Images │ Product │ │\r│(7 APIs) │(3 APIs) │(3 APIs) │(1 API) │ Details │ │\r│ │ │ │ │(7 APIs) │ │\r└─────────┴─────────┴─────────┴─────────┴─────────┴───────────────┘\r↓ ↓ ↓ ↓ ↓\rRDS SQL Server S3 Images\r(via Secrets Manager) Step 1: Review Project Structure Lambda Functions Structure:\nOJT_lambda/\r├── shared/ # Shared utilities\r│ ├── database.js # RDS connection helper\r│ ├── auth.js # JWT utilities\r│ └── response.js # API response formatters\r├── auth/ # Authentication (4 functions)\r│ ├── login.js # POST /auth/login\r│ ├── signup.js # POST /auth/signup\r│ ├── logout.js # POST /auth/logout\r│ └── me.js # GET /auth/me\r├── products/ # Products (12 functions)\r│ ├── getProducts.js # GET /products\r│ ├── getProductDetail.js # GET /products/detail/{id}\r│ ├── createProduct.js # POST /products\r│ ├── updateProduct.js # PUT /products/{id}\r│ ├── deleteProduct.js # DELETE /products/{id}\r│ ├── searchProducts.js # GET /products/search\r│ ├── getBestSelling.js # GET /products/best-selling\r│ ├── getNewest.js # GET /products/newest\r│ └── ...\r├── cart/ # Cart (6 functions)\r├── orders/ # Orders (9 functions)\r├── categories/ # Categories (6 functions)\r├── brands/ # Brands (5 functions)\r├── banners/ # Banners (7 functions)\r├── ratings/ # Ratings (3 functions)\r├── users/ # Users (3 functions)\r├── images/ # Images (1 function)\r└── product-details/ # Product Details (7 functions) Step 2: Configure Lambda Environment Variables 1. Navigate to Lambda project\ncd OJT_lambda 2. Copy environment template\ncopy .env.example .env 3. Edit .env file\n# AWS Configuration AWS_REGION=ap-southeast-1 AWS_ACCOUNT_ID=123456789012 # Database Configuration (from CDK outputs) DB_HOST=ojt-database.xxx.ap-southeast-1.rds.amazonaws.com DB_NAME=demoaws DB_SECRET_ARN=arn:aws:secretsmanager:ap-southeast-1:123456789012:secret:OJT/RDS/Credentials # JWT Configuration JWT_SECRET=your-jwt-secret-key JWT_EXPIRES_IN=7d # S3 Images Bucket (from CDK outputs) S3_IMAGES_BUCKET=ojt-ecommerce-images-123456789012 4. Get values from CDK outputs\n# Get RDS endpoint aws rds describe-db-instances ` --db-instance-identifier ojt-database ` --query \u0026#39;DBInstances[0].Endpoint.Address\u0026#39; ` --output text # Get Secrets Manager ARN aws secretsmanager list-secrets ` --query \u0026#34;SecretList[?contains(Name, \u0026#39;OJT\u0026#39;)].ARN\u0026#34; ` --output text # Get S3 bucket name aws s3 ls | Select-String \u0026#34;ojt-ecommerce-images\u0026#34; Step 3: Review API Endpoints Authentication APIs (4 endpoints):\nMethod Endpoint Handler Description POST /auth/login login.js User login POST /auth/signup signup.js User registration POST /auth/logout logout.js User logout GET /auth/me me.js Get current user Products APIs (12 endpoints):\nMethod Endpoint Handler Description GET /products getProducts.js List all products GET /products/detail/{id} getProductDetail.js Product detail POST /products createProduct.js Create product (Admin) PUT /products/{id} updateProduct.js Update product (Admin) DELETE /products/{id} deleteProduct.js Delete product (Admin) GET /products/search searchProducts.js Search products GET /products/best-selling getBestSelling.js Best selling products GET /products/newest getNewest.js Newest products GET /products/category/{id} getProductsByCategory.js Products by category GET /products/brand/{id} getProductsByBrand.js Products by brand GET /products/price-range getProductsByPriceRange.js Products by price Cart APIs (6 endpoints):\nMethod Endpoint Handler Description POST /cart addToCart.js Add to cart GET /cart/me getMyCart.js Get user\u0026rsquo;s cart PUT /cart/{id} updateCartItem.js Update cart item DELETE /cart/{id} removeCartItem.js Remove cart item DELETE /cart clearCart.js Clear cart GET /cart/count getCartCount.js Get cart count Orders APIs (9 endpoints):\nMethod Endpoint Handler Description GET /orders getAllOrders.js All orders (Admin) POST /orders createOrder.js Create order POST /orders/create-cod createOrderCOD.js Create COD order GET /orders/{id}/details getOrderDetails.js Order details GET /orders/user/{userId} getUserOrders.js User\u0026rsquo;s orders PATCH /orders/{id}/status updateOrderStatus.js Update status DELETE /orders/{id} cancelOrder.js Cancel order Step 4: Install Lambda Dependencies # Install main dependencies cd OJT_lambda npm install # Install all module dependencies npm run install:all This installs dependencies for:\nshared/ - Database, auth, response utilities auth/ - bcryptjs, jsonwebtoken products/ - Database queries cart/, orders/, etc. Step 5: Review Shared Utilities Database Helper (shared/database.js):\nconst sql = require(\u0026#39;mssql\u0026#39;); const config = { server: process.env.DB_HOST, database: process.env.DB_NAME, user: process.env.DB_USERNAME, password: process.env.DB_PASSWORD, options: { encrypt: true, trustServerCertificate: true } }; async function query(sqlQuery, params = []) { const pool = await sql.connect(config); const result = await pool.request(); // Add parameters params.forEach((param, index) =\u0026gt; { result.input(`param${index}`, param); }); return result.query(sqlQuery); } module.exports = { query, sql }; Auth Helper (shared/auth.js):\nconst jwt = require(\u0026#39;jsonwebtoken\u0026#39;); function generateToken(user) { return jwt.sign( { userId: user.UserID, email: user.Email, role: user.Role }, process.env.JWT_SECRET, { expiresIn: process.env.JWT_EXPIRES_IN || \u0026#39;7d\u0026#39; } ); } function verifyToken(token) { return jwt.verify(token, process.env.JWT_SECRET); } module.exports = { generateToken, verifyToken }; Response Helper (shared/response.js):\nfunction success(data, statusCode = 200) { return { statusCode, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(data) }; } function error(message, statusCode = 500) { return { statusCode, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: message }) }; } module.exports = { success, error }; Step 6: Verify API Gateway Configuration 1. Get API Gateway URL from CDK outputs\n# Get API Gateway URL aws cloudformation describe-stacks ` --stack-name OJT-ApiStack ` --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue\u0026#39; ` --output text Expected output:\nhttps://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod 2. Check API Gateway resources\n# Get API ID $API_ID = aws apigateway get-rest-apis ` --query \u0026#34;items[?contains(name, \u0026#39;OJT\u0026#39;)].id\u0026#34; ` --output text # List resources aws apigateway get-resources --rest-api-id $API_ID Step 7: Verify Lambda Functions 1. List Lambda functions\naws lambda list-functions ` --query \u0026#34;Functions[?contains(FunctionName, \u0026#39;OJT-Ecommerce\u0026#39;)].FunctionName\u0026#34; ` --output table Expected output:\nOJT-Ecommerce-AuthModule\rOJT-Ecommerce-ProductsModule\rOJT-Ecommerce-ProductDetailsModule\rOJT-Ecommerce-CartModule\rOJT-Ecommerce-OrdersModule\rOJT-Ecommerce-CategoriesModule\rOJT-Ecommerce-BrandsModule\rOJT-Ecommerce-BannersModule\rOJT-Ecommerce-RatingsModule\rOJT-Ecommerce-UsersModule\rOJT-Ecommerce-ImagesModule 2. Check Lambda environment variables\naws lambda get-function-configuration ` --function-name OJT-Ecommerce-AuthModule ` --query \u0026#39;Environment.Variables\u0026#39; Step 8: Test Lambda Functions Locally 1. Test Auth Login\ncd OJT_lambda # Test login handler node -e \u0026#34; const handler = require(\u0026#39;./auth/login.js\u0026#39;).handler; const event = { body: JSON.stringify({ email: \u0026#39;test@test.com\u0026#39;, password: \u0026#39;Test123!\u0026#39; }) }; handler(event).then(console.log); \u0026#34; 2. Test Products List\n# Test get products handler node -e \u0026#34; const handler = require(\u0026#39;./products/getProducts.js\u0026#39;).handler; const event = { queryStringParameters: { page: \u0026#39;1\u0026#39;, limit: \u0026#39;10\u0026#39; } }; handler(event).then(console.log); \u0026#34; Step 9: Build Lambda Packages # Build all Lambda packages npm run build # Or build specific module npm run build:auth npm run build:products This creates ZIP files in build/ directory:\nbuild/\r├── auth.zip\r├── products.zip\r├── product-details.zip\r├── cart.zip\r├── orders.zip\r├── categories.zip\r├── brands.zip\r├── banners.zip\r├── ratings.zip\r├── users.zip\r└── images.zip Configuration Checklist Lambda project structure reviewed Environment variables configured in .env Database connection details obtained from CDK outputs JWT secret configured S3 bucket name configured Dependencies installed (npm install + npm run install:all) Shared utilities reviewed (database, auth, response) API Gateway URL obtained Lambda functions listed and verified Local tests passing Lambda packages built (npm run build) API Summary Module Functions Endpoints Auth 4 login, signup, logout, me Products 12 CRUD + search, filter, best-selling, newest Product Details 7 CRUD + images upload Cart 6 add, get, update, remove, clear, count Orders 9 CRUD + COD, status, date-range Categories 6 CRUD + search Brands 5 CRUD Banners 7 CRUD + toggle Ratings 3 get, stats, create Users 3 getAll, getById, updateProfile Images 1 upload Total 63 Next Steps Once configuration is complete, proceed to [Deploy Backend] to deploy your Lambda code to AWS.\n"
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building Furious Five Fashion: AWS Full-Stack Infrastructure Workshop Overview The system architecture is built on a full-stack serverless model on AWS, focusing on automatic scalability, multi-layer security and cost optimization. All frontend – backend – data – AI – security components operate in a private environment, connected through VPC, PrivateLink and AWS management services\nYou will deploy seven CDK stacks linked together to create a scalable, secure and cost-optimized application:\nFrontend Layer – Deploy the interface on Amplify and distribute content via CloudFront.\nRouting \u0026amp; Protection – Protect access with Route 53, WAF and ACM SSL certificates.\nAuthentication Layer – Create a Cognito User Pool and integrate authentication for API Gateway.\nAPI Layer – Build a private API Gateway to securely communicate with the backend.\nCompute Layer – Run business logic using Lambda functions inside a private VPC.\nStorage Layer – Store static data and uploads on S3 via VPC Endpoint.\nData Layer – Run RDS in a private subnet and control access using IAM/SG.\nAI Layer – Integrate Amazon Bedrock to handle AI tasks via PrivateLink.\nSecurity \u0026amp; Observability – Monitor the entire system using CloudWatch, send alerts via SNS and manage security using IAM.\nContent Workshop Overview Setup Environment CDK Bootstrap Configure Infrastructure Stacks Configure API \u0026amp; Lambda Deploy Backend Services Test Endpoints End-to-End Push to GitLab Clean up "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.06-deploy-backend/",
	"title": "Deploy Backend Services",
	"tags": [],
	"description": "",
	"content": "Overview Dự án OJT E-commerce sử dụng kiến trúc 2-step deployment: Infrastructure (CDK) và Lambda Code (riêng biệt). Trong bước này, bạn sẽ deploy cả infrastructure và Lambda code.\nDeployment Architecture ┌─────────────────────────────────────────────────────────────┐\r│ 2-Step Deployment │\r├─────────────────────────────────────────────────────────────┤\r│ Step 1: Deploy Infrastructure (CDK) │\r│ ├─ NetworkStack: VPC, Subnets, NAT Gateway │\r│ ├─ StorageStack: S3 Buckets │\r│ ├─ AuthStack: Cognito (optional) │\r│ ├─ DatabaseStack: RDS SQL Server │\r│ ├─ ApiStack: API Gateway + Placeholder Lambda │\r│ ├─ FrontendStack: S3 + CloudFront │\r│ └─ MonitoringStack: CloudWatch │\r├─────────────────────────────────────────────────────────────┤\r│ Step 2: Deploy Lambda Code │\r│ └─ 11 Lambda Modules (63 APIs) → Update function code │\r└─────────────────────────────────────────────────────────────┘ Lambda Modules (11 modules - 63 APIs) Module Functions Description Auth 4 Login, Signup, Logout, Me Products 12 CRUD, Search, Filter ProductDetails 7 CRUD, Images Cart 6 Add, Get, Update, Remove Orders 9 CRUD, COD, Status Categories 6 CRUD, Search Brands 5 CRUD Banners 7 CRUD, Toggle Ratings 3 Get, Stats, Create Users 3 GetAll, GetById, Update Images 1 Upload to S3 Step 1: Deploy Infrastructure (CDK) 1.1 Navigate to Infrastructure Directory cd OJT_infrastructure 1.2 Install Dependencies npm install 1.3 Build TypeScript npm run build 1.4 Deploy Core Stacks # Deploy Network, Storage, Auth, Database stacks npm run deploy:core Expected output:\nOJT-NetworkStack\rOJT-StorageStack\rOJT-AuthStack\rOJT-DatabaseStack\rOutputs:\rOJT-NetworkStack.VpcId = vpc-0123456789abcdef0\rOJT-DatabaseStack.DbEndpoint = ojt-database.xxx.ap-southeast-1.rds.amazonaws.com\rOJT-StorageStack.ImagesBucketName = ojt-ecommerce-images-123456789012 Deploy time: ~15-20 minutes (RDS takes longest)\nScreenshot: CDK deploying core stacks\n1.5 Deploy API Stack # Deploy API Gateway + Placeholder Lambda npm run deploy:api Expected output:\n✅ OJT-ApiStack\rOutputs:\rOJT-ApiStack.ApiUrl = https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod\rOJT-ApiStack.AuthModuleName = OJT-Ecommerce-AuthModule\rOJT-ApiStack.ProductsModuleName = OJT-Ecommerce-ProductsModule\r... Deploy time: ~3-5 minutes\nScreenshot: CDK deploying API stack\n1.6 Verify CDK Deployment # List all deployed stacks aws cloudformation list-stacks ` --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE ` --query \u0026#34;StackSummaries[?contains(StackName, \u0026#39;OJT\u0026#39;)].StackName\u0026#34; ` --output table Expected output:\n---------------------------------\r| ListStacks |\r+-------------------------------+\r| OJT-NetworkStack |\r| OJT-StorageStack |\r| OJT-AuthStack |\r| OJT-DatabaseStack |\r| OJT-ApiStack |\r+-------------------------------+ Step 2: Deploy Lambda Code Sau khi CDK deploy xong, Lambda functions có placeholder code. Bây giờ deploy actual code.\n2.1 Navigate to Lambda Directory cd ..\\OJT_lambda 2.2 Install Dependencies # Install main dependencies npm install # Install all module dependencies npm run install:all 2.3 Configure Environment # Copy environment template copy .env.example .env # Edit .env with CDK outputs notepad .env Update .env with values from CDK outputs:\n# AWS Configuration AWS_REGION=ap-southeast-1 AWS_ACCOUNT_ID=123456789012 # Database (from OJT-DatabaseStack outputs) DB_HOST=ojt-database.xxx.ap-southeast-1.rds.amazonaws.com DB_NAME=demoaws DB_SECRET_ARN=arn:aws:secretsmanager:ap-southeast-1:123456789012:secret:OJT/RDS/Credentials # JWT JWT_SECRET=your-jwt-secret-key JWT_EXPIRES_IN=7d # S3 (from OJT-StorageStack outputs) S3_IMAGES_BUCKET=ojt-ecommerce-images-123456789012 2.4 Build Lambda Packages # Build all Lambda packages npm run build Expected output:\nBuilding auth module... Done\rBuilding products module... Done\rBuilding product-details module... Done\rBuilding cart module... Done\rBuilding orders module... Done\rBuilding categories module... Done\rBuilding brands module... Done\rBuilding banners module... Done\rBuilding ratings module... Done\rBuilding users module... Done\rBuilding images module... Done\rBuild completed! ZIP files in build/ directory. Build creates:\nbuild/\r├── auth.zip (~500 KB)\r├── products.zip (~600 KB)\r├── product-details.zip (~550 KB)\r├── cart.zip (~450 KB)\r├── orders.zip (~500 KB)\r├── categories.zip (~400 KB)\r├── brands.zip (~350 KB)\r├── banners.zip (~400 KB)\r├── ratings.zip (~350 KB)\r├── users.zip (~400 KB)\r└── images.zip (~300 KB) 2.5 Deploy Lambda Code # Deploy all Lambda functions npm run deploy Expected output:\nDeploying auth module to OJT-Ecommerce-AuthModule... Done\rDeploying products module to OJT-Ecommerce-ProductsModule... Done\rDeploying product-details module to OJT-Ecommerce-ProductDetailsModule... Done\rDeploying cart module to OJT-Ecommerce-CartModule... Done\rDeploying orders module to OJT-Ecommerce-OrdersModule... Done\rDeploying categories module to OJT-Ecommerce-CategoriesModule... Done\rDeploying brands module to OJT-Ecommerce-BrandsModule... Done\rDeploying banners module to OJT-Ecommerce-BannersModule... Done\rDeploying ratings module to OJT-Ecommerce-RatingsModule... Done\rDeploying users module to OJT-Ecommerce-UsersModule... Done\rDeploying images module to OJT-Ecommerce-ImagesModule... Done\rAll Lambda functions deployed successfully! Deploy time: ~1-2 minutes\nScreenshot: Lambda code deployment\nStep 3: Verify Lambda Deployment 3.1 List Lambda Functions aws lambda list-functions ` --query \u0026#34;Functions[?contains(FunctionName, \u0026#39;OJT-Ecommerce\u0026#39;)].{Name:FunctionName,Runtime:Runtime,Updated:LastModified}\u0026#34; ` --output table Expected output:\n--------------------------------------------------------------------\r| ListFunctions |\r+----------------------------------+------------+------------------+\r| Name | Runtime | Updated |\r+----------------------------------+------------+------------------+\r| OJT-Ecommerce-AuthModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-ProductsModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-ProductDetailsModule | nodejs20.x | 2025-12-09T...|\r| OJT-Ecommerce-CartModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-OrdersModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-CategoriesModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-BrandsModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-BannersModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-RatingsModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-UsersModule | nodejs20.x | 2025-12-09T... |\r| OJT-Ecommerce-ImagesModule | nodejs20.x | 2025-12-09T... |\r+----------------------------------+------------+------------------+ 3.2 Check Function Configuration # Check Auth Module configuration aws lambda get-function-configuration ` --function-name OJT-Ecommerce-AuthModule ` --query \u0026#39;{Runtime:Runtime,Handler:Handler,Timeout:Timeout,Memory:MemorySize}\u0026#39; Expected output:\n{ \u0026#34;Runtime\u0026#34;: \u0026#34;nodejs20.x\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;index.handler\u0026#34;, \u0026#34;Timeout\u0026#34;: 30, \u0026#34;Memory\u0026#34;: 128 } 3.3 Verify Code Updated # Check code SHA256 (changes when code updates) aws lambda get-function ` --function-name OJT-Ecommerce-AuthModule ` --query \u0026#39;Configuration.CodeSha256\u0026#39; Step 4: Test Lambda Functions 4.1 Test Auth Login # Invoke Auth Module aws lambda invoke ` --function-name OJT-Ecommerce-AuthModule ` --payload \u0026#39;{\\\u0026#34;httpMethod\\\u0026#34;:\\\u0026#34;POST\\\u0026#34;,\\\u0026#34;path\\\u0026#34;:\\\u0026#34;/auth/login\\\u0026#34;,\\\u0026#34;body\\\u0026#34;:\\\u0026#34;{\\\\\\\u0026#34;email\\\\\\\u0026#34;:\\\\\\\u0026#34;test@test.com\\\\\\\u0026#34;,\\\\\\\u0026#34;password\\\\\\\u0026#34;:\\\\\\\u0026#34;Test123!\\\\\\\u0026#34;}\\\u0026#34;}\u0026#39; ` response.json # Check response Get-Content response.json 4.2 Test Products List # Invoke Products Module aws lambda invoke ` --function-name OJT-Ecommerce-ProductsModule ` --payload \u0026#39;{\\\u0026#34;httpMethod\\\u0026#34;:\\\u0026#34;GET\\\u0026#34;,\\\u0026#34;path\\\u0026#34;:\\\u0026#34;/products\\\u0026#34;,\\\u0026#34;queryStringParameters\\\u0026#34;:{\\\u0026#34;page\\\u0026#34;:\\\u0026#34;1\\\u0026#34;,\\\u0026#34;limit\\\u0026#34;:\\\u0026#34;10\\\u0026#34;}}\u0026#39; ` response.json # Check response Get-Content response.json 4.3 Test via API Gateway # Get API URL $apiUrl = aws cloudformation describe-stacks ` --stack-name OJT-ApiStack ` --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue\u0026#39; ` --output text # Test products endpoint Invoke-RestMethod -Uri \u0026#34;$apiUrl/products\u0026#34; -Method Get Step 5: Check CloudWatch Logs 5.1 Tail Logs Real-time # Tail Auth Module logs aws logs tail /aws/lambda/OJT-Ecommerce-AuthModule --follow 5.2 View Recent Logs # View last 10 minutes aws logs tail /aws/lambda/OJT-Ecommerce-AuthModule --since 10m 5.3 Search for Errors # Search for errors aws logs filter-log-events ` --log-group-name /aws/lambda/OJT-Ecommerce-AuthModule ` --filter-pattern \u0026#34;ERROR\u0026#34; Screenshot: CloudWatch Logs showing Lambda execution\nStep 6: Deploy Frontend \u0026amp; Monitoring (Optional) 6.1 Deploy Frontend Stack cd ..\\OJT_infrastructure npm run deploy:frontend 6.2 Deploy Monitoring Stack npm run deploy:monitoring Deployment Summary Time Estimates Step Time Notes CDK Deploy Core 15-20 min RDS takes longest CDK Deploy API 3-5 min API Gateway + Lambda Lambda Build 1-2 min ZIP packages Lambda Deploy 1-2 min Update function code Total ~25 min First deployment Update Lambda Code Only Khi chỉ thay đổi Lambda code (không thay đổi infrastructure):\ncd OJT_lambda # Build and deploy (skip CDK) npm run build npm run deploy # Time: ~2-3 minutes Deployment Checklist Infrastructure (CDK) NetworkStack deployed (VPC, Subnets, NAT Gateway) StorageStack deployed (S3 Buckets) AuthStack deployed (Cognito - optional) DatabaseStack deployed (RDS SQL Server) ApiStack deployed (API Gateway + Placeholder Lambda) Lambda Code Dependencies installed (npm install + npm run install:all) Environment configured (.env file) Lambda packages built (npm run build) Lambda code deployed (npm run deploy) Verification All 11 Lambda functions listed Runtime: nodejs20.x Code SHA256 updated Test invocations successful API Gateway responding CloudWatch logs showing executions Troubleshooting Issue: CDK Deploy Fails # Check AWS credentials aws sts get-caller-identity # Check CDK version cdk --version # Clean and rebuild Remove-Item -Recurse -Force node_modules, cdk.out npm install npm run build Issue: Lambda Deploy Fails # Check function exists aws lambda list-functions | Select-String \u0026#34;OJT-Ecommerce\u0026#34; # Check ZIP file created Get-ChildItem build/*.zip # Rebuild and redeploy npm run build npm run deploy Issue: Function Timeout # Increase timeout aws lambda update-function-configuration ` --function-name OJT-Ecommerce-AuthModule ` --timeout 60 Issue: Database Connection Error # Verify RDS endpoint aws rds describe-db-instances ` --db-instance-identifier ojt-database ` --query \u0026#39;DBInstances[0].Endpoint\u0026#39; # Check Security Group allows Lambda aws ec2 describe-security-groups ` --filters \u0026#34;Name=group-name,Values=*OJT*RDS*\u0026#34; Next Steps Backend đã deployed thành công! Tiếp theo:\nTest Endpoints: Verify tất cả 63 API endpoints → Deploy Frontend: React application → Monitor: CloudWatch dashboards → "
},
{
	"uri": "http://localhost:1313/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [ Amazon Web Services Vietnam Co., Ltd] from [8/9/2025] to [12/9/2025], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [First Cloud Journey], through which I improved my skills in [list skills: programming, teamwork, cloud computing, problem-solving, critical thinking, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Discipline: Improve punctuality and consistency with schedules.\nActions: set daily start/end times, use a task tracker (e.g., Notion/Trello) and log work start time for 2 weeks. Success metric: aim for 95% on-time starts within 4 weeks. Ability to Learn: Move from \u0026ldquo;Fair\u0026rdquo; to \u0026ldquo;Good\u0026rdquo; by learning faster and applying knowledge.\nActions: after each learning session, write a 10–15 minute summary and a short checklist of 3 applied tasks; complete one small project applying newly learned concepts every 2 weeks. Success metric: produce 4 applied summaries/projects in 8 weeks. Communication: Improve clarity in status updates and technical explanations.\nActions: use a 2-line status template (What I did / Blockers / Next) in daily standups and practice explaining one technical task in writing to a peer weekly. Success metric: reduce follow-up clarifying questions from reviewers by 50% in 6 weeks. Problem-Solving: Strengthen analytical approach and debugging skills.\nActions: adopt a root-cause checklist (reproduce → isolate → hypothesize → test → confirm) for each bug; keep a short \u0026ldquo;bug log\u0026rdquo; summarizing cause and fix. Success metric: average time-to-fix critical bugs reduced by 25% over the next month. Overall Professional Growth: Convert feedback into visible improvement.\nActions: schedule biweekly feedback reviews with your mentor, set 2 measurable goals per review, and document progress. Success metric: demonstrate measurable progress on at least 2 goals by the next review cycle. "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.07-test-endpoints/",
	"title": "Test Endpoints End-to-End",
	"tags": [],
	"description": "",
	"content": "Overview Sau khi deploy backend thành công, bạn cần test tất cả API endpoints để đảm bảo hệ thống hoạt động đúng. Workshop này hướng dẫn chi tiết cách test từng module của hệ thống OJT E-commerce.\nAPI Architecture API Gateway (REST API)\r↓\r┌─────────────────────────────────────────────────────────────────┐\r│ 11 Lambda Modules (63 APIs) │\r├─────────┬─────────┬─────────┬─────────┬─────────┬───────────────┤\r│ Auth │Products │ Cart │ Orders │Categories│ Brands │\r│(4 APIs) │(12 APIs)│(6 APIs) │(9 APIs) │ (6 APIs) │ (5 APIs) │\r├─────────┼─────────┼─────────┼─────────┼─────────┼───────────────┤\r│ Banners │ Ratings │ Users │ Images │ Product │ │\r│(7 APIs) │(3 APIs) │(3 APIs) │(1 API) │ Details │ │\r│ │ │ │ │(7 APIs) │ │\r└─────────┴─────────┴─────────┴─────────┴─────────┴───────────────┘\r↓\rRDS SQL Server (via Secrets Manager) Step 1: Get API Endpoint 1. Get API URL from CloudFormation Outputs\n# Get API endpoint from API Stack $API_URL = aws cloudformation describe-stacks ` --stack-name OJT-ApiStack ` --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApiUrl`].OutputValue\u0026#39; ` --output text Write-Host \u0026#34;API Endpoint: $API_URL\u0026#34; # Output: https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod 2. Setup Environment Variables\n# Set API endpoint for PowerShell session $API_URL = \u0026#34;https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod\u0026#34; Write-Host \u0026#34;API Endpoint configured: $API_URL\u0026#34; Step 2: Test Authentication APIs 2.1 Test User Signup # POST /auth/signup $signupBody = @{ email = \u0026#34;testuser@example.com\u0026#34; password = \u0026#34;Test123!\u0026#34; fullName = \u0026#34;Test User\u0026#34; phone = \u0026#34;+84901234567\u0026#34; } | ConvertTo-Json $response = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/auth/signup\u0026#34; ` -Method Post ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $signupBody $response | ConvertTo-Json 2.2 Test User Login # POST /auth/login $loginBody = @{ email = \u0026#34;testuser@example.com\u0026#34; password = \u0026#34;Test123!\u0026#34; } | ConvertTo-Json $loginResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/auth/login\u0026#34; ` -Method Post ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $loginBody # Save JWT token for authenticated requests $TOKEN = $loginResponse.token Write-Host \u0026#34;JWT Token: $TOKEN\u0026#34; $loginResponse | ConvertTo-Json # Expected: { \u0026#34;token\u0026#34;: \u0026#34;eyJhbG...\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Customer\u0026#34; } } 2.3 Test Get Current User # GET /auth/me (requires authentication) $headers = @{ \u0026#34;Authorization\u0026#34; = \u0026#34;Bearer $TOKEN\u0026#34; } $meResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/auth/me\u0026#34; ` -Method Get ` -Headers $headers $meResponse | ConvertTo-Json # Expected: { \u0026#34;user\u0026#34;: { \u0026#34;userId\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;testuser@example.com\u0026#34;, \u0026#34;fullName\u0026#34;: \u0026#34;Test User\u0026#34; } } Step 3: Test Products APIs 3.1 Get All Products # GET /products $products = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products?page=1\u0026amp;limit=10\u0026#34; ` -Method Get Write-Host \u0026#34;Found $($products.total) products\u0026#34; $products | ConvertTo-Json -Depth 3 3.2 Get Product Detail # GET /products/detail/{id} $productId = 1 $product = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/detail/$productId\u0026#34; ` -Method Get $product | ConvertTo-Json -Depth 3 3.3 Search Products # GET /products/search?q=keyword $searchResults = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/search?q=phone\u0026#34; ` -Method Get $searchResults | ConvertTo-Json -Depth 3 3.4 Get Best Selling Products # GET /products/best-selling $bestSelling = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/best-selling?limit=10\u0026#34; ` -Method Get $bestSelling | ConvertTo-Json -Depth 3 3.5 Get Newest Products # GET /products/newest $newest = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/newest?limit=10\u0026#34; ` -Method Get $newest | ConvertTo-Json -Depth 3 3.6 Get Products by Category # GET /products/category/{id} $categoryId = 1 $categoryProducts = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/category/$categoryId\u0026#34; ` -Method Get $categoryProducts | ConvertTo-Json -Depth 3 3.7 Get Products by Brand # GET /products/brand/{id} $brandId = 1 $brandProducts = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/products/brand/$brandId\u0026#34; ` -Method Get $brandProducts | ConvertTo-Json -Depth 3 Step 4: Test Cart APIs 4.1 Add to Cart # POST /cart (requires authentication) $cartBody = @{ productDetailId = 1 quantity = 2 } | ConvertTo-Json $cartResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart\u0026#34; ` -Method Post ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $cartBody $cartResponse | ConvertTo-Json 4.2 Get My Cart # GET /cart/me $myCart = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart/me\u0026#34; ` -Method Get ` -Headers $headers $myCart | ConvertTo-Json -Depth 3 4.3 Update Cart Item # PUT /cart/{id} $cartItemId = 1 $updateBody = @{ quantity = 3 } | ConvertTo-Json $updateResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart/$cartItemId\u0026#34; ` -Method Put ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $updateBody $updateResponse | ConvertTo-Json 4.4 Get Cart Count # GET /cart/count $cartCount = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart/count\u0026#34; ` -Method Get ` -Headers $headers Write-Host \u0026#34;Cart items: $($cartCount.count)\u0026#34; 4.5 Remove Cart Item # DELETE /cart/{id} Invoke-RestMethod ` -Uri \u0026#34;$API_URL/cart/$cartItemId\u0026#34; ` -Method Delete ` -Headers $headers Write-Host \u0026#34;Cart item removed\u0026#34; Step 5: Test Orders APIs 5.1 Create Order (COD) # POST /orders/create-cod $orderBody = @{ shippingAddress = \u0026#34;123 Test Street, District 1, Ho Chi Minh City\u0026#34; phone = \u0026#34;+84901234567\u0026#34; note = \u0026#34;Please call before delivery\u0026#34; } | ConvertTo-Json $orderResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/orders/create-cod\u0026#34; ` -Method Post ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $orderBody $ORDER_ID = $orderResponse.orderId Write-Host \u0026#34;Order created: $ORDER_ID\u0026#34; $orderResponse | ConvertTo-Json 5.2 Get Order Details # GET /orders/{id}/details $orderDetails = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/orders/$ORDER_ID/details\u0026#34; ` -Method Get ` -Headers $headers $orderDetails | ConvertTo-Json -Depth 3 5.3 Get User Orders # GET /orders/user/{userId} $userId = 1 $userOrders = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/orders/user/$userId\u0026#34; ` -Method Get ` -Headers $headers $userOrders | ConvertTo-Json -Depth 3 5.4 Update Order Status (Admin) # PATCH /orders/{id}/status $statusBody = @{ status = \u0026#34;Processing\u0026#34; } | ConvertTo-Json $statusResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/orders/$ORDER_ID/status\u0026#34; ` -Method Patch ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $statusBody $statusResponse | ConvertTo-Json Step 6: Test Categories APIs 6.1 Get All Categories # GET /categories $categories = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/categories\u0026#34; ` -Method Get $categories | ConvertTo-Json -Depth 3 6.2 Get Category by ID # GET /categories/{id} $categoryId = 1 $category = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/categories/$categoryId\u0026#34; ` -Method Get $category | ConvertTo-Json 6.3 Search Categories # GET /categories/search?q=keyword $categorySearch = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/categories/search?q=phone\u0026#34; ` -Method Get $categorySearch | ConvertTo-Json Step 7: Test Brands APIs 7.1 Get All Brands # GET /brands $brands = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/brands\u0026#34; ` -Method Get $brands | ConvertTo-Json -Depth 3 7.2 Get Brand by ID # GET /brands/{id} $brandId = 1 $brand = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/brands/$brandId\u0026#34; ` -Method Get $brand | ConvertTo-Json Step 8: Test Banners APIs 8.1 Get All Banners # GET /banners $banners = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/banners\u0026#34; ` -Method Get $banners | ConvertTo-Json -Depth 3 8.2 Get Active Banners # GET /banners?active=true $activeBanners = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/banners?active=true\u0026#34; ` -Method Get $activeBanners | ConvertTo-Json -Depth 3 Step 9: Test Ratings APIs 9.1 Get Product Ratings # GET /ratings/product/{id} $productId = 1 $ratings = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/ratings/product/$productId\u0026#34; ` -Method Get $ratings | ConvertTo-Json -Depth 3 9.2 Get Rating Stats # GET /ratings/product/{id}/stats $ratingStats = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/ratings/product/$productId/stats\u0026#34; ` -Method Get $ratingStats | ConvertTo-Json # Expected: { \u0026#34;averageRating\u0026#34;: 4.5, \u0026#34;totalRatings\u0026#34;: 10, \u0026#34;distribution\u0026#34;: { \u0026#34;5\u0026#34;: 5, \u0026#34;4\u0026#34;: 3, \u0026#34;3\u0026#34;: 2 } } 9.3 Create Rating # POST /ratings $ratingBody = @{ productId = 1 rating = 5 comment = \u0026#34;Great product! Highly recommended.\u0026#34; } | ConvertTo-Json $ratingResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/ratings\u0026#34; ` -Method Post ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $ratingBody $ratingResponse | ConvertTo-Json Step 10: Test Users APIs (Admin) 10.1 Get All Users # GET /users (Admin only) $users = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/users\u0026#34; ` -Method Get ` -Headers $headers $users | ConvertTo-Json -Depth 3 10.2 Get User by ID # GET /users/{id} $userId = 1 $user = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/users/$userId\u0026#34; ` -Method Get ` -Headers $headers $user | ConvertTo-Json 10.3 Update User Profile # PUT /users/profile/{id} $profileBody = @{ fullName = \u0026#34;Updated Name\u0026#34; phone = \u0026#34;+84909876543\u0026#34; address = \u0026#34;456 New Street, District 2\u0026#34; } | ConvertTo-Json $profileResponse = Invoke-RestMethod ` -Uri \u0026#34;$API_URL/users/profile/$userId\u0026#34; ` -Method Put ` -Headers $headers ` -ContentType \u0026#34;application/json\u0026#34; ` -Body $profileBody $profileResponse | ConvertTo-Json Step 11: Test Images Upload API 11.1 Upload Image # POST /images/upload # Note: This requires multipart/form-data # Using curl for file upload curl -X POST \u0026#34;$API_URL/images/upload\u0026#34; ` -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; ` -F \u0026#34;file=@D:\\test-image.jpg\u0026#34; ` -F \u0026#34;type=product\u0026#34; --- ### Step 12: Verify CloudWatch Logs #### 12.1 Check Lambda Logs ```powershell # Tail Auth Module logs aws logs tail /aws/lambda/OJT-Ecommerce-AuthModule --follow # Tail Products Module logs aws logs tail /aws/lambda/OJT-Ecommerce-ProductsModule --follow # View last 10 minutes aws logs tail /aws/lambda/OJT-Ecommerce-AuthModule --since 10m Testing Checklist Authentication APIs (4 endpoints) POST /auth/signup - User registration POST /auth/login - User login, get JWT token POST /auth/logout - User logout GET /auth/me - Get current user Products APIs (12 endpoints) GET /products - List all products GET /products/detail/{id} - Get product detail GET /products/search - Search products GET /products/best-selling - Best selling products GET /products/newest - Newest products GET /products/category/{id} - Products by category GET /products/brand/{id} - Products by brand GET /products/price-range - Products by price range POST /products - Create product (Admin) PUT /products/{id} - Update product (Admin) DELETE /products/{id} - Delete product (Admin) Cart APIs (6 endpoints) POST /cart - Add to cart GET /cart/me - Get my cart PUT /cart/{id} - Update cart item DELETE /cart/{id} - Remove cart item DELETE /cart - Clear cart GET /cart/count - Get cart count Orders APIs (9 endpoints) POST /orders - Create order POST /orders/create-cod - Create COD order GET /orders/{id}/details - Get order details GET /orders/user/{userId} - Get user orders GET /orders - Get all orders (Admin) PATCH /orders/{id}/status - Update order status DELETE /orders/{id} - Cancel order Categories APIs (6 endpoints) GET /categories - List all categories GET /categories/{id} - Get category by ID GET /categories/search - Search categories POST /categories - Create category (Admin) PUT /categories/{id} - Update category (Admin) DELETE /categories/{id} - Delete category (Admin) Brands APIs (5 endpoints) GET /brands - List all brands GET /brands/{id} - Get brand by ID POST /brands - Create brand (Admin) PUT /brands/{id} - Update brand (Admin) DELETE /brands/{id} - Delete brand (Admin) Banners APIs (7 endpoints) GET /banners - List all banners GET /banners/{id} - Get banner by ID GET /banners?active=true - Get active banners POST /banners - Create banner (Admin) PUT /banners/{id} - Update banner (Admin) DELETE /banners/{id} - Delete banner (Admin) PATCH /banners/{id}/toggle - Toggle banner (Admin) Ratings APIs (3 endpoints) GET /ratings/product/{id} - Get product ratings GET /ratings/product/{id}/stats - Get rating stats POST /ratings - Create rating Users APIs (3 endpoints) GET /users - Get all users (Admin) GET /users/{id} - Get user by ID PUT /users/profile/{id} - Update profile Images API (1 endpoint) POST /images/upload - Upload image Performance Benchmarks Expected Response Times:\nEndpoint Expected Time Notes POST /auth/login \u0026lt; 500ms JWT generation GET /products \u0026lt; 300ms Database query GET /products/detail/{id} \u0026lt; 200ms Single record POST /cart \u0026lt; 300ms Database write POST /orders/create-cod \u0026lt; 500ms Transaction GET /categories \u0026lt; 200ms Cached data POST /images/upload \u0026lt; 2s S3 upload "
},
{
	"uri": "http://localhost:1313/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSharing and Feedback on Internship Experience Summary — Overall Impressions 1. Working environment\nThe team is welcoming and supportive; colleagues and mentors are generous with time and guidance. The workspace is clean and conducive to focused work. Consider adding periodic team-building or social events to strengthen interpersonal connections. 2. Mentorship \u0026amp; administrative support\nMentors provide clear, practical guidance and encourage hands-on problem solving. Administrative support is responsive and helpful in arranging onboarding and documentation. 3. Relevance to academic learning\nAssigned tasks closely relate to academic foundations while introducing practical tools and workflows, bridging theory and practice effectively. 4. Learning opportunities\nThe internship offered growth in technical skills, project management tools, collaboration, and professional communication. Mentor insights added valuable real-world context. 5. Culture \u0026amp; teamwork\nThe culture promotes mutual respect and collaboration; team members step up during high-pressure moments and include interns in meaningful work. 6. Policies \u0026amp; benefits\nThe internship includes an allowance, flexible scheduling when needed, and access to internal training — all of which enhance the learning experience. Short Feedback Prompts What was the most satisfying part of your internship? What should the company improve for future interns? Would you recommend this internship to a friend? Why or why not? Suggestions \u0026amp; Expectations Do you have suggestions to improve the internship structure or onboarding? Are you interested in continuing with the program or joining full-time after completion? Any additional comments or reflections? "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.08-push-gitlab/",
	"title": "Push to GitLab",
	"tags": [],
	"description": "",
	"content": "Overview Sau khi test thành công, bạn sẽ push code lên GitLab để version control và chuẩn bị cho CI/CD. Dự án OJT E-commerce có 3 phần chính cần quản lý: Infrastructure, Lambda, và Frontend.\nProject Structure OJT/\r├── OJT_infrastructure/ # CDK Infrastructure (TypeScript)\r├── OJT_lambda/ # Lambda Functions (JavaScript)\r├── OJT_frontendDev/ # Frontend (React + Vite)\r├── database/ # Database Scripts\r└── README.md # Project documentation Step 1: Initialize Git Repository 1. Navigate to Project Root\ncd D:\\AWS\\Src\\OJT 2. Check Git Status\n# Check if Git is already initialized git status # If not initialized: git init 3. Configure Git\n# Set your name and email git config user.name \u0026#34;Your Name\u0026#34; git config user.email \u0026#34;your.email@example.com\u0026#34; # Verify configuration git config --list Step 2: Create .gitignore 1. Create .gitignore File\n# Create .gitignore in project root @\u0026#34; # Dependencies node_modules/ package-lock.json # Build outputs dist/ build/ *.js.map cdk.out/ # Environment variables .env .env.local .env.*.local # AWS *.pem *.key # Deployment packages *.zip # Logs logs/ *.log npm-debug.log* # IDE .vscode/settings.json .idea/ *.swp *.swo # OS .DS_Store Thumbs.db # CDK cdk.context.json # TypeScript *.tsbuildinfo *.d.ts *.js !vite.config.js !eslint.config.js # Lambda build OJT_lambda/build/ \u0026#34;@ | Out-File -FilePath .gitignore -Encoding UTF8 2. Verify .gitignore\nGet-Content .gitignore Step 3: Stage and Commit Files 1. Add Files to Staging\n# Add all files git add . # Check what will be committed git status 2. Review Files to Commit\n# List files to be committed git diff --cached --name-only # Expected files: # OJT_infrastructure/ # OJT_lambda/ # OJT_frontendDev/ # database/ # README.md # .gitignore 3. Create Initial Commit\n# Commit with message git commit -m \u0026#34;Initial commit: OJT E-commerce infrastructure and backend\u0026#34; # Verify commit git log --oneline ### Step 4: Create GitLab Repository **1. Login to GitLab** Go to https://gitlab.com/ and login with your account. **2. Create New Project** 1. Click **\u0026#34;New project\u0026#34;** 2. Choose **\u0026#34;Create blank project\u0026#34;** 3. Fill in details: - Project name: `ojt-ecommerce` - Project slug: `ojt-ecommerce` - Visibility: **Private** (recommended) - Initialize with README: **No** (we already have code) 4. Click **\u0026#34;Create project\u0026#34;** **3. Copy Repository URL** https://gitlab.com/your-username/ojt-ecommerce.git\n---\r### Step 5: Add Remote and Push\r**1. Add GitLab Remote**\r```powershell\r# Add remote origin\rgit remote add origin https://gitlab.com/your-username/ojt-ecommerce.git\r# Verify remote\rgit remote -v 2. Push to GitLab\n# Push to main branch git push -u origin main # If your default branch is \u0026#39;master\u0026#39;: git branch -M main git push -u origin main 3. Enter Credentials\nWhen prompted:\nUsername: Your GitLab username Password: Your GitLab Personal Access Token (not password) Screenshot: Terminal showing successful push\nStep 6: Create Personal Access Token If you don\u0026rsquo;t have a Personal Access Token:\n1. Go to GitLab Settings\nClick your avatar → \u0026ldquo;Preferences\u0026rdquo; Go to \u0026ldquo;Access Tokens\u0026rdquo; in left sidebar 2. Create New Token\nToken name: OJT-Ecommerce-Token Expiration date: Set appropriate date Select scopes: read_repository write_repository Click \u0026ldquo;Create personal access token\u0026rdquo; 3. Save Token\nCopy and save the token securely. You won\u0026rsquo;t be able to see it again.\nStep 7: Verify Push on GitLab 1. Check Repository on GitLab\nGo to your repository: https://gitlab.com/your-username/ojt-ecommerce\n2. Verify Files\nCheck that all files are uploaded:\nOJT_infrastructure/ - CDK Infrastructure OJT_lambda/ - Lambda Functions OJT_frontendDev/ - Frontend database/ - Database Scripts README.md - Documentation .gitignore - Git ignore rules Step 8: Create Development Branch 1. Create dev Branch\n# Create and switch to dev branch git checkout -b dev # Push dev branch to GitLab git push -u origin dev 2. Set Default Branch (Optional)\nOn GitLab:\nGo to Settings → Repository Under Branch defaults, set dev as default branch Step 9: Setup Branch Protection (Optional) 1. Protect Main Branch\nOn GitLab:\nGo to Settings → Repository → Protected branches Add main branch: Allowed to merge: Maintainers Allowed to push: No one Require approval: Yes 2. Protect Dev Branch\nAdd dev branch with similar settings but allow developers to push.\nGit Workflow Summary 1. Create feature branch from dev\rgit checkout dev\rgit pull origin dev\rgit checkout -b feature/new-feature\r2. Make changes and commit\rgit add .\rgit commit -m \u0026#34;feat: Add new feature\u0026#34;\r3. Push to GitLab\rgit push -u origin feature/new-feature\r4. Create Merge Request on GitLab\rfeature/new-feature → dev\r5. Review and merge\r6. Deploy to dev environment\rcd OJT_lambda\rnpm run build\rnpm run deploy\r7. Test in dev environment\r8. Merge dev → main for production Commit Message Convention Follow conventional commits:\nfeat: Add user authentication\rfix: Fix login validation bug\rdocs: Update API documentation\rstyle: Format code with prettier\rrefactor: Refactor database queries\rtest: Add unit tests for auth\rchore: Update dependencies\rperf: Optimize product search query Examples:\ngit commit -m \u0026#34;feat: Add cart functionality\u0026#34; git commit -m \u0026#34;fix: Fix order total calculation\u0026#34; git commit -m \u0026#34;docs: Update README with deployment steps\u0026#34; Branch Naming Convention feature/add-cart-api\rfeature/user-authentication\rbugfix/fix-login-error\rbugfix/cart-quantity-issue\rhotfix/critical-security-patch\rrelease/v1.0.0 Troubleshooting Issue: Authentication Failed # Use Personal Access Token instead of password # When prompted for password, enter your token # Or configure credential helper git config --global credential.helper store Issue: Large Files Rejected # Check file sizes git ls-files -z | ForEach-Object { $size = (Get-Item $_).Length / 1MB if ($size -gt 1) { Write-Host \u0026#34;$_ : $size MB\u0026#34; } } # Add large files to .gitignore echo \u0026#34;*.zip\u0026#34; \u0026gt;\u0026gt; .gitignore git rm --cached *.zip git commit -m \u0026#34;Remove large files\u0026#34; Issue: Push Rejected (Non-Fast-Forward) # Pull latest changes first git pull origin main --rebase # Then push git push origin main Issue: Merge Conflicts # Update your branch git checkout feature/your-feature git fetch origin git merge origin/dev # Resolve conflicts in files # Then commit git add . git commit -m \u0026#34;Resolve merge conflicts\u0026#34; git push Repository Checklist Git initialized in project root .gitignore created with proper exclusions Initial commit created GitLab repository created Remote origin added Code pushed to GitLab Personal Access Token created (if needed) Files verified on GitLab Dev branch created Branch protection configured (optional) Next Steps "
},
{
	"uri": "http://localhost:1313/workshop-template/5-workshop/5.09-cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Overview When you complete the workshop and no longer want to continue using it, delete all resources to avoid incurring charges.\nWarning: This process cannot be undone. All data will be permanently deleted.\nResources to Delete OJT E-commerce project includes:\nStack Resources Monthly Cost MonitoringStack CloudWatch Dashboard, Alarms ~$1.50 FrontendStack S3, CloudFront ~$1.50 ApiStack API Gateway, 11 Lambda functions ~$2 DatabaseStack RDS SQL Server, Secrets Manager ~$15 AuthStack Cognito User Pool $0 StorageStack S3 Buckets ~$1.25 NetworkStack VPC, NAT Gateway ~$23 Total ~$44/month Cleanup Order Must delete in reverse order of deployment:\n1. MonitoringStack\r2. FrontendStack\r3. ApiStack\r4. DatabaseStack\r5. AuthStack\r6. StorageStack\r7. NetworkStack\r8. CDK Bootstrap (optional) Step 1: Navigate to Infrastructure Directory cd D:\\AWS\\Src\\OJT\\OJT_infrastructure Step 2: Empty S3 Buckets S3 buckets must be emptied before deletion:\n# List all OJT buckets aws s3 ls | Select-String \u0026#34;ojt\u0026#34; # Empty Images bucket aws s3 rm s3://ojt-ecommerce-images-123456789012 --recursive # Empty Logs bucket aws s3 rm s3://ojt-ecommerce-logs-123456789012 --recursive # Empty Frontend bucket (if deployed) aws s3 rm s3://ojt-ecommerce-frontend-123456789012 --recursive Or use PowerShell script:\n# Get all OJT buckets and empty them $buckets = aws s3 ls | Select-String \u0026#34;ojt\u0026#34; | ForEach-Object { $_.ToString().Split()[-1] } foreach ($bucket in $buckets) { Write-Host \u0026#34;Emptying bucket: $bucket\u0026#34; -ForegroundColor Yellow aws s3 rm \u0026#34;s3://$bucket\u0026#34; --recursive Write-Host \u0026#34;Bucket emptied: $bucket\u0026#34; -ForegroundColor Green } Step 3: Delete CDK Stacks 3.1 Delete Monitoring Stack # Delete Monitoring stack npx cdk destroy OJT-MonitoringStack --force # Or with confirmation npx cdk destroy OJT-MonitoringStack # Type \u0026#39;y\u0026#39; to confirm 3.2 Delete Frontend Stack # Delete Frontend stack npx cdk destroy OJT-FrontendStack --force 3.3 Delete API Stack # Delete API stack (Lambda functions + API Gateway) npx cdk destroy OJT-ApiStack --force 3.4 Delete Database Stack # Delete Database stack (RDS SQL Server) # This takes 5-10 minutes npx cdk destroy OJT-DatabaseStack --force Note: RDS deletion creates a final snapshot by default.\n3.5 Delete Auth Stack # Delete Auth stack (Cognito) npx cdk destroy OJT-AuthStack --force 3.6 Delete Storage Stack # Delete Storage stack (S3 buckets) npx cdk destroy OJT-StorageStack --force 3.7 Delete Network Stack # Delete Network stack (VPC, NAT Gateway) # This takes 3-5 minutes npx cdk destroy OJT-NetworkStack --force Step 4: Delete All Stacks at Once (Alternative) # Delete all stacks in correct order npm run destroy # Or using CDK directly npx cdk destroy --all --force Expected output:\nOJT-MonitoringStack: destroying...\rOJT-MonitoringStack: destroyed\rOJT-FrontendStack: destroying...\rOJT-FrontendStack: destroyed\rOJT-ApiStack: destroying...\rOJT-ApiStack: destroyed\rOJT-DatabaseStack: destroying...\rOJT-DatabaseStack: destroyed\rOJT-AuthStack: destroying...\rOJT-AuthStack: destroyed\rOJT-StorageStack: destroying...\rOJT-StorageStack: destroyed\rOJT-NetworkStack: destroying...\rOJT-NetworkStack: destroyed Step 5: Verify All Stacks Deleted # List remaining stacks aws cloudformation list-stacks ` --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE ` --query \u0026#34;StackSummaries[?contains(StackName, \u0026#39;OJT\u0026#39;)].StackName\u0026#34; ` --output table # Should return empty or no OJT stacks Step 6: Delete CDK Bootstrap (Optional) ⚠️ Only do this if you\u0026rsquo;re done with CDK completely in this region:\n# Get CDK assets bucket name $BUCKET_NAME = aws s3 ls | Select-String \u0026#34;cdk-\u0026#34; | ForEach-Object { $_.ToString().Split()[-1] } # Empty CDK assets bucket aws s3 rm \u0026#34;s3://$BUCKET_NAME\u0026#34; --recursive # Delete CDK bootstrap stack aws cloudformation delete-stack --stack-name CDKToolkit # Wait for deletion aws cloudformation wait stack-delete-complete --stack-name CDKToolkit Write-Host \u0026#34;CDK Bootstrap deleted\u0026#34; -ForegroundColor Green Step 7: Verify Complete Cleanup 7.1 Check CloudFormation aws cloudformation list-stacks ` --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE ` | Select-String \u0026#34;OJT\u0026#34; # Should return nothing 7.2 Check S3 aws s3 ls | Select-String \u0026#34;ojt\u0026#34; # Should return nothing 7.3 Check Lambda aws lambda list-functions ` --query \u0026#34;Functions[?contains(FunctionName, \u0026#39;OJT\u0026#39;)].FunctionName\u0026#34; # Should return empty array 7.4 Check RDS aws rds describe-db-instances ` --query \u0026#34;DBInstances[?contains(DBInstanceIdentifier, \u0026#39;ojt\u0026#39;)].DBInstanceIdentifier\u0026#34; # Should return empty array 7.5 Check API Gateway aws apigateway get-rest-apis ` --query \u0026#34;items[?contains(name, \u0026#39;OJT\u0026#39;)].name\u0026#34; # Should return empty array 7.6 Check Cognito aws cognito-idp list-user-pools --max-results 10 ` --query \u0026#34;UserPools[?contains(Name, \u0026#39;OJT\u0026#39;)].Name\u0026#34; # Should return empty array 7.7 Check NAT Gateway aws ec2 describe-nat-gateways ` --filter \u0026#34;Name=state,Values=available\u0026#34; ` --query \u0026#34;NatGateways[].NatGatewayId\u0026#34; # Should not show OJT NAT Gateways Step 8: Delete RDS Snapshots (Optional) # List RDS snapshots aws rds describe-db-snapshots ` --query \u0026#34;DBSnapshots[?contains(DBSnapshotIdentifier, \u0026#39;ojt\u0026#39;)].DBSnapshotIdentifier\u0026#34; # Delete each snapshot aws rds delete-db-snapshot --db-snapshot-identifier ojt-database-final-snapshot Step 9: Delete CloudWatch Log Groups (Optional) # List log groups aws logs describe-log-groups ` --log-group-name-prefix /aws/lambda/OJT ` --query \u0026#34;logGroups[].logGroupName\u0026#34; # Delete each log group $logGroups = aws logs describe-log-groups ` --log-group-name-prefix /aws/lambda/OJT ` --query \u0026#34;logGroups[].logGroupName\u0026#34; ` --output text foreach ($logGroup in $logGroups.Split()) { Write-Host \u0026#34;Deleting log group: $logGroup\u0026#34; aws logs delete-log-group --log-group-name $logGroup } Cost After Cleanup Immediate:\nMost resources: $0/month RDS snapshots: ~$0.095/GB/month (if kept) After complete cleanup:\nEverything: $0/month Troubleshooting Cleanup Issue: S3 Bucket Deletion Fails # Force empty and delete aws s3 rb s3://bucket-name --force Issue: CloudFormation Stack Stuck in DELETE_IN_PROGRESS # Check stack events for errors aws cloudformation describe-stack-events ` --stack-name OJT-NetworkStack ` --max-items 10 # If stuck, wait or check for dependencies Issue: RDS Deletion Protection Enabled # Disable deletion protection aws rds modify-db-instance ` --db-instance-identifier ojt-database ` --no-deletion-protection ` --apply-immediately # Wait a few minutes, then retry delete Issue: VPC Has Dependencies # Check for remaining ENIs aws ec2 describe-network-interfaces ` --filters \u0026#34;Name=vpc-id,Values=vpc-xxxxxxxx\u0026#34; # Delete any remaining ENIs manually aws ec2 delete-network-interface --network-interface-id eni-xxxxxxxx Issue: NAT Gateway Still Exists # Delete NAT Gateway manually aws ec2 delete-nat-gateway --nat-gateway-id nat-xxxxxxxx # Release Elastic IP aws ec2 release-address --allocation-id eipalloc-xxxxxxxx Cleanup Checklist CDK Stacks MonitoringStack deleted FrontendStack deleted ApiStack deleted DatabaseStack deleted AuthStack deleted StorageStack deleted NetworkStack deleted AWS Resources All S3 buckets emptied and deleted No remaining Lambda functions No remaining RDS instances No remaining API Gateways No remaining Cognito User Pools No remaining NAT Gateways No remaining VPCs Optional Cleanup CDK Bootstrap deleted RDS snapshots deleted CloudWatch Log Groups deleted GitLab repository archived/deleted Verification CloudFormation shows no OJT stacks AWS Billing shows decreasing costs No unexpected charges Conclusion You have completed the OJT E-commerce workshop! You have learned:\nInfrastructure as Code with AWS CDK (TypeScript)\nServerless Architecture with Lambda and API Gateway\nRDS SQL Server database management\nVPC Design with public/private/isolated subnets\nS3 Storage for images and static files\nCloudFront CDN with Origin Access Control\nCognito Authentication (optional)\nJWT Authentication with custom Lambda\nCloudWatch Monitoring with dashboards and alarms\nCost Optimization strategies for serverless\nGitLab version control\n2-Step Deployment strategy (Infrastructure + Lambda code)\nTotal deployed:\n7 CDK stacks 11 Lambda modules (63 APIs) RDS SQL Server database VPC with NAT Gateway S3 buckets for storage CloudFront CDN CloudWatch monitoring Production-ready e-commerce platform Estimated monthly cost: ~$44/month (optimized)\nThank you for completing the workshop! 🎉\n"
},
{
	"uri": "http://localhost:1313/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]